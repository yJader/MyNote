# Lecture 1: Introduction

## 1.1 分布式概述

### why分布式

1. 提高计算性能(increase capacity) → 通过多台计算机(cpu, 主存, 磁盘)并行
2. 容错(tolerate faults)
3. 很多实际上的需求就是需要连接物理上分隔的机器
4. 通过分布式运行代码, 来保证安全

### challenge

1. 有许多的并行执行 → 人脑很难理解并行
2. 分布式的组成太多, 错误情况复杂
3. 如何衡量分布式的性能 1000台机子能达到多少的性能



## 1.2 课程(lab)介绍

1. MapReduce
2. Raft算法实现容错
3. 基于Raft的KV
4. 分片式KV (并行加速)



## 1.3 分布式系统的抽象和实现工具（Abstraction and Implementation）

分布式系统的基础架构(infrastructure)

- 存储: 如何去构建一种多副本，容错的，高性能分布式存储实现
- 通信(网络)
- 计算

构建分布式系统需要对底层这些基础设施进行封装和抽象, 让用户使用接口时, 像是在使用非分布式的系统



### implement

**RPC**: Remote Procedure Call

**线程**: 用线程这个结构化的概念去简化并发操作

并发控制--**锁**

## 1.4 可扩展性（Scalability）

> 即为性能的拓展

理想的可拓展性: 两倍的机器 两倍的执行速度(一半的计算耗时)

另一种提升性能的方式: 重构系统, 优化算法, 但是更为耗时且expensive

- 比如我的网站在一个时刻突然爆火, 此时想要重构去优化已经来不及了, 想保证网站的可用只能购买更多服务器

## 1.5 可用性（Availability）

> 即为系统的容错

对于单计算机构建的系统, 通常是可靠的(计算机, os, 电源一般不会出错); 

但是多台计算机, 罕见的问题会被放大

- > 例如在我们的1000台计算机的集群中，总是有故障，要么是机器故障，要么是运行出错，要么是运行缓慢，要么是执行错误的任务。一个更常见的问题是网络，在一个有1000台计算机的网络中，会有大量的网络电缆和网络交换机，所以总是会有人踩着网线导致网线从接口掉出，或者交换机风扇故障导致交换机过热而不工作。在一个大规模分布式系统中，各个地方总是有一些小问题出现。所以大规模系统会将一些几乎不可能并且你不需要考虑的问题，变成一个持续不断的问题。

- 所以, 我们在设计时就要保证系统在出错之后能够屏蔽和掩盖错误

==可用性==(Availability): 在特定的故障范围内，系统仍然能够提供服务，系统仍然是可用的。如果出现了更多的故障，系统将不再可用。

- 比如构建了多副本系统

==可恢复性==(recoverability): 如果出现了问题，服务会停止工作，不再响应请求，之后有人来修复，并且在修复之后系统仍然可以正常运行，就像没有出现过问题一样。

- 这是一个**比可用性更弱**的需求，因为在出现故障到故障组件被修复期间，系统将会完全停止工作。但是修复之后，系统又可以完全正确的重新运行，所以可恢复性是一个重要的需求。

- 为了实现可恢复, 需要将系统数据进行backup, 以便之后恢复, 常用工具如下
  - 非易失存储（non-volatile storage，类似于硬盘）
  - 复制（replication）: 但是管理复制的副本, 使其状态同步较为复杂(lab2&3会涉及到)

## 1.6 一致性（Consistency）

> 假设构建了一套KV的分布式存储, 有两个操作, get和put
>
> put操作会将一个value存入一个key；get操作会取出key对应的value

对于分布式系统, 因为存在复制/缓存, 数据会存在于多个副本之中

- 加入初始时, AB机器维护了相同的KV表, 有<1, 100>
- 对机器A put<1, 200>, 但在此时, AB之间通信出错, B并没有收到put
- 之后get请求可能从A读, 得到200; 或是从B读, 得到100
  - 还有可能get一直从A读, 数据都是正确的, 直到某次A故障, 此时我们只保存了B中错误的100

### 强/弱一致

**强一致性**: get请求可以得到最近一次完成的put请求写入的值

**弱一致性**: 不保证get请求可以得到最近一次完成的put请求写入的值

强一致固然正确, 但是想要维护它需要带来很高的 用于同步的通信开销(不管put还是get都要同时读写所有副本, 并使用最新的数据)

- 存储我们的副本的物理位置通常较远(异地副本), 如果保持强一致, 物理距离带来的通信时延本身就很高, 大大拖慢系统速度

所以弱一致系统更为常用



## 1.7 MapReduce基本工作方式

> 背景: 论文2004年发布, 当时需要在TB级别上进行计算, 如对所有网页创建索引, 创建索引相当于对数据进行排序, 这个庞大的计算量需要通过将计算运行在大量的机器上
>
> 如果只雇佣熟练的分布式系统专家去编写分布式软件来实现每一种需求, 会有些浪费, 所以提出了MapReduce框架, 让工程师只需要实现应用的核心, 不需要考虑分布式下计算机的协作和错误处理

MapReduce的思想是，应用程序设计人员和分布式运算的使用者，只需要写简单的Map函数和Reduce函数，而不需要知道任何有关分布式的事情，MapReduce框架会处理剩下的事情。

### MapReduce抽象结构

> 以统计单词频率为例

输入: 被分割成几块的输入文件

1. MapReduce框架查找Map函数, 然后对每个Input执行Map函数
   - <img src="./Note.assets/image-20240725221635955.png" alt="image-20240725221635955" style="zoom:50%;" />
   - 得到了多个`<key, value>`键值对, 其中key是word, value是1, 表示频率
   - *根据论文提到的, 在这之后可以执行一个额外的combiner, 来合并计数结果*

2. 将<key, value>中间结果分组, 发送给reduce函数执行
   - <img src="./Note.assets/image-20240725234329701.png" alt="image-20240725234329701" style="zoom:50%;" />
   - 第一个Reduce输出a=2，第二个Reduce输出b=2，第三个Reduce输出c=1

**一些概念**: 

- **Job**。整个MapReduce计算称为Job。
- **Task**。每一次MapReduce调用称为Task。



## 1.8 Map函数和Reduce函数

### Map函数

Map函数的**输入参数**为key, value

- key是输入文件的名字(一般忽略)
- value是要统计的文本, 我们要将他拆成单词

对于每个word, 会调用由MapReduce框架提供的`emit`, 其中参数为`(key, value)`其中key是单词, value是字符串“1”

- emit是Map的一部分

> that's all, 非常简单, 只需要描述计算机需要做什么, 不用考虑底层的传输/计算调度方式

### Reduce

Reduce函数**输入参数**为某个中间结果key的所有实例, 所以同样是`(key, value)`

- 对于例子中的WordCount, key即为word, value即为同一单词多次出现后, 多次emit的产生的由**字符串“1”构成的数组**

Reduce同样有一个emit, 这里的输入参数只有`value`, 这个`value`对应输入参数中`key`的最终输出结果

### Other

#### 能否将Reduce输出传给另一个Map

Reduce输出一堆value, 这和Map输入一致

此外, 现实中复杂的多阶段分析or迭代算法本身就需要多个MapReduce Job来实现

> 对于一些非常复杂的多阶段分析或者迭代算法，比如说Google用来评价网页的重要性和影响力的PageRank算法，这些算法是逐渐向答案收敛的。
>
> 我认为Google最初就是这么使用MapReduce的，他们运行MapReduce Job多次，每一次的输出都是一个网页的列表，其中包含了网页的价值，权重或者重要性。所以将MapReduce的输出作为另一个MapReduce Job的输入这很正常。

#### 将Reduce输出作为Map输入, Reduce输出时需要注意什么

需要在输出时考虑到下一个MapReduce Job的输入格式来生成数据

这也是MapReduce框架的缺陷: 算法最好能够由Map函数、Map函数的中间输出以及Reduce函数来表达

此外, Map函数是独立的, 只会和入参相关, 如果需要比较长的运算, 就必须组合多个Map

#### emit函数本质

每次Map调用emit, worker就会将数据写入本地磁盘

#### MapReduce框架中的数据传输

论文中提到, 存储的文件都保存在分布式存储GFS中, 如果每次Input文件都通过**网络传输**给worker节点, 通信开销过大(如10T通过论文中GB级的带宽, 时延很大)

所以论文中提到, 通过修改GFS和worker架构来减少网络传输

- 具体是将GFS和worker混合运行在一组服务器上, master在拆分Map任务时会查找文件存储在哪个服务器, 然后将任务分配给那个服务器(如果不能, 则选择最近的)

同样的, Map输出写到本地磁盘, 不需要(实时的)网络通信

但是将Map传递给Reduce需要通过key来分类(这个过程称为Shuffle 混洗)并传输

**重提: 网络传输是MapReduce的瓶颈(2004年)**

- 对于多个MapReduce的Job, 10T输入通常对应着10T的输出, 输出需要写到GFS中
- 此外GFS为了提高性能和容错, 还会对数据进行拆分和拷贝(2-3副本), 这会带来更多的网络通信

但在现代数据中心网络中, 会有很多的root交换机做负载分担(而不只是论文中的1个), 吞吐大大增加

- > 我认为Google几年前就不再使用MapReduce了，不过在那之前，现代的MapReduce已经不再尝试在GFS数据存储的服务器上运行Map函数了，它乐意从任何地方加载数据，因为网络已经足够快了。