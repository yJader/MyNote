1. 简答: 3~4题 20分
   - 规范化
   - 文本分类
   - 熵
   - 算法区别
2. 计算: 80分 参考作业
   1. A, FP, 关联规则
   2. ID3, KNN, ID3、KNN、BAYES
   3. KMeans算法， 距离计算
   4. 回归（相关性）
   5. 文本分类--PageRank



# DM4 关联规则

## 4.1 关联规则基本概念

1. ==关联规则==(Association Rule)可以表示为一个蕴含式： $R：X\Rightarrow Y$ 
   - 例如：$R：尿布\Rightarrow啤酒$

2. **项集和频繁项集**
   - 项目的集合称为==项集==，项目个数为K的项集称为==K项集==
     例如，{牛奶，面包，黄油}是个3项集
   - 支持度大于或等于**最小支持度**的项集称为==频繁项集==，反之则称为非频繁项集。
   - 如果k项集 满足最小支持度，称为==频繁k项集==，记作$L_k$。

3. ==支持度==

   - 关联规则R的支持度(support)是交易集中同时包含X和Y的交易数与**所有交易数**之比。
   - $support(X\Rightarrow Y) = \frac{count(X\cup Y)}{D} = P(X\cup Y)$
   - P: 概率

4. ==置信度==

   - 关联规则R的置信度(confidence)是指同时包含X和Y的交易数与**包含X的交易数**之比
   - $confidence(X\Rightarrow Y) = \frac{count(X\cup Y)}{count(x)} = \frac{P(X\cup Y)}{P(X)} = P(Y|X)$

5. 关联规则的==最小支持度==和==最小置信度==

   - 关联规则的最小支持度也就是衡量频繁集的最小支持度(Minimum Support)，记为 supmin，它用于衡量规则需要满足的最低重要性。 

   - 关联规则的最小置信度(Minimum Confidence)记为confmin，它用于衡量规则需要满足 的最低可靠性。 

6. ==强关联规则==

   - 如果规则$R:X\Rightarrow Y$满足$\begin{cases}support(X\Rightarrow Y)\geq supmin\  \\ confidence(X\Rightarrow Y)\geq confmin\end{cases}$，称关联规则$X\Rightarrow Y$为强关联规则，否则称关联规则$X\Rightarrow Y$为弱关联规则。 
   - 在挖掘关联规则时，产生的关联规则要经过supmin和confmin衡量，筛选出的强关联规 则才能用于指导商家的决策。



## 4.3 Apriori算法

### 4.3.1 算法描述

Apriori算法由连接和剪枝两个步骤组成

1. 连接：为了找$L_k$ ，通过$L_{k-1}$与自己连接产生候选k项集的集合，该候选k项集记为$C_k$ 。 
   - $L_{k-1}$中的两个元素$l_1$和$l_2$可以执行连接操作$l_1 l_2$的条件是
     $(l_1[1]=l_2[1])\and (l_1[2]=l_2[2])\and ...\and (l_1[k-2]=l_2[k-2]) \and \textcolor{red}{(l_1[k-1]<l_2[k-1])}$
   - $Ck=L_{k-1}\connect L_{k-1}$ 
2. 剪枝：对候选k项集Ck，删除其非频繁的选项，得到Lk
   1. Ck是Lk的超集（Lk Ck ），即它的成员可能不是频繁的，但是所有频繁的都在Ck中。 因此可以通过扫描数据库，侯选项集的计数值不小于最小支持度计数就是频繁的，属 于Lk ； 
   2. 为了减少计算量，可以使用Apriori性质，即如果k项集的一个(k-1)子集不在Lk-1中， 则该候选不可能是频繁的，可以直接从Ck删除。



### 4.3.2 Apriori算法调优

Apriori算法的主要挑战 

- 可能需要重复扫描数据库： 如果频繁集最多包含10个项，那么就需要扫描交易数据表10遍，这需要很大的I/O负载； 
- 可能产生大量的候选项集： 若有100个项目，可能产生候选项集数： 
- 对候选项集的支持度计算非常繁琐； 

解决思路 

- 减少对数据的扫描次数； 
- 缩小产生的候选项集； 
- 改进对候选项集的支持度计算方法

#### ① 散列





# DM5 决策树

## 5.2 决策树归纳

### **信息增益**

$Info(D)=-\sum_{\\i=1}^nP_ilogP_i$ 对所有样本(D)进行分类

$Info_A(D)=\sum_{j=1\\}^{V} \frac{|D_j|}{|D|}Info(D_j)$ 对根据A进行分类后的样本(D~j~)进行分类

$Gain(A)=Info(D)-Info_A(D)$

选择具有最高信息增益的属性进行分裂

### 增益率

分裂信息: $SplitInfo_A(D)=-\sum_{j=1\\}^V\frac{|D_j|}{|D|}Info(\frac{|D_j|}{|D|})$

增益率: $GainRate(A)=\frac{Gain(A)}{SplitInfo_A(D)}$ 

### 基尼指数

$Gini(D)=1-\sum_{\\i=1}^mp_i^2$

$Gini_A(D)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$

不纯度降低=$\Delta Gini(A)=Gini(D)-Gini_A(D)$
