# 考试范围

## ver1

1. 简答: 3~4题 20分
   - 规范化
   - 文本分类
   - 熵
   - 算法区别
2. 计算: 80分 参考作业
   1. A, FP, 关联规则
   2. ID3, KNN, ID3、KNN、BAYES
   3. KMeans算法， 距离计算
   4. 回归（相关性）
   5. 文本分类--PageRank

## ver2

上课内容: 到DM7-聚类 && 回归

题型: 选择 简答 

1. dm1
   - 无具体计算, 可以看看数据挖掘流程, 开放计算题会用到
   - 数据挖掘&KDD: 数据挖掘是核心
   - 数据挖掘& 数据仓库: ←提供数据 & →技术
     - 数据仓库dm0不考
   - 什么类型的数据(dm2) && 什么类型的模式()
2. dm2 认识数据
   1. 属性类型: 二元/数值/标称 (结合第四小节的计算)
      - 计算都是数值型
   2. 基本统计描述
      - 不需要计算, 了解概念
      - 均值(小声说了句没有到)和方差
      - 什么是中位数, 众数, 应用到什么类型的数据(结合数据属性)
   3. 无可视化
   4. 相似性和相异性的**计算**
      - 二元的临近性, 计算跟例题差不多
      - 标准化(规范化)
      - 度量方式: 只有**欧几里得(欧式), 曼哈顿, 夹角余弦距离**
      - 最大最小计算, 了解一下小数
   5. 作业: 做!
3. dm3 数据预处理
   1. **清理**
      - 
   2. 集成
      - 相关性分析(结合回归分析), 看一下题目
        - 相关系数: 正相关, 独立, 负相关
   3. **规约**
      - 什么是主成分分析, kl变换, 流形学习 [了解一下]
      - 选择和提取
        - 如何选择: 启发式, 利用方差&标准差
   4. 离散化和概念分成(略)
   5. 变换

4. dm4 **关联规则**
   1. apriori算法 特点 不足 计算
   2. fp算法 特点 计算
   3. apriori和fp比较: 支持度低apriori挖掘出的频繁项集更多, 所以更慢
   4. 作业 例题6.3, 6.5
      1. fp注意多路径
5. dm5 分类
   1. **计算&特点**: 决策树, 贝叶斯
   2. 决策树
      1. 三种信息选择度量 特点, 联系和区别(ppt29页)
         1. **信息增益: 计算**信息熵, 信息增益
         2. 增益率: 
         3. 基尼指数
   3. 贝叶斯
      1. 考试形式类似作业
      2. 特点和不足, 如何克服
   4. 模型评估与选择
      1. 分类器的**评估度量 **重点
         1. 混淆矩阵
      2. ?
      3. ?
6. dm6
   1. sum, 贝叶斯, bp, knn: 了解特点和联系, 不用计算
7. dm7
   - 划分, 层次 **计算**
   - 密度 了解特点

# DM1 引论

## 1.1 为什么进行数据挖掘？

数据挖掘的意义: 从大量数据中发掘interesting的**知识和模式**



## 1.2 什么是数据挖掘？

从数据库中挖掘知识: 

![image-20240412140435629](./数据挖掘.assets/image-20240412140435629.png)

- 数据清理(消除噪声和删除不一致的数据。占全过程60％的工作量)
- 数据集成（多种数据源可以组合在一起）
- 数据选择（从数据库中提取与分析任务相关的数据）
- 数据变换（数据变换或统一成适合挖掘的形式）
- **数据挖掘（核心步骤，使用智能方法提取数据模式）**
- 模式评估（根据某种兴趣度度量，识别提供知识的真正有趣的模式）
- 知识表示（使用可视化和知识表示技术，向用户提供挖掘的知识）

## 1.3 挖掘什么类型的数据？

1. 数据库系统
   - 数据库系统由一组内部相关的数据（数据库）和一组管理和存取数据的软件程序组成
2. 数据仓库
   - 数据仓库是一个从多个数据源收集的信息存储库，存放在一致的模式下，并且通常驻留在单个站点上。
3. 

## 1.4 挖掘什么类型的模式及如何挖掘？

## 1.5 数据挖掘流程及应用

![image-20240412141244589](./数据挖掘.assets/image-20240412141244589.png)



## 1.6 相关研究领域



# DM2 认识数据

## 2.1 数据对象与属性类型

### 2.1.1 数据对象

==数据对象==（又称样本）：它代表一个实体，由一组属性（或特征/变量/维） 描述，描述一个样本的一组属性称为属性向量（或特征向量）

- 样本: $X = (x_1, x_2,...,x_n)$
- 属性: $x_i$, 可为各种类型, 可取各种值

### 2.1.2 属性类型

1. ==标称属性== 
   - 定性的，属性值是一些符号或事物的名称
     - 如 hair_color: 可能为黑色, 白色, 棕色…
   - 不定量, 不定序
2. ==二元属性== 
   - 二元属性是一种标称属性，只有两个类别或状态：0或1
   - 如果两种状态对应于 true和false的话，二元属性又称**布尔属性**。
3. **序数属性**
   - 定性的，序数属性值之间具有有意义的序或秩评定，但是相继值之间的差是未知的，其中心趋势可以用众数和中位数来表示
     - 只有排序, 具体两级差多少未知
4. ==数值属性==
   - 数值属性可以是区间标度 的或比率标度的，其中心趋势度量可以用均值、中位数或众数来表示。
   - 定量的，即它是可度量的量，用**整数或实数值**表示。
5. 离散的或连续的（机器学习领域分类算法的常用分类法） 
   - 离散属性具有有限或无限可能个值，可以用或不用整数表示。如：属性 hair_color、smoker都有有限个值，因此是离散的。 
   - 如果属性不是离散的，则它是连续的。

## 2.2 数据的基本统计描述

1. 动机：能够理好地理解数据，它们对于数据清理特别有用 

   - 获得数据的总体印象 

   - 识别数据的典型特征 

   - 凸显噪声或离群点 

2. 度量数据的中心趋势 

   - **均值、中位数、众数（模）等** 

3. 度量数据的离散程度 

   - 极差、四分位数、四分位数极差、方差、标准差等 

4. 基本统计描述的图形显示 

   - 盒图、分位数图、Q-Q图、直方图、散点图、局部回归曲线

### 2.2.1 数据的中心趋势

#### ==均值==

==均值== $\overline{x} = \frac1n\sum_{i=1}^nx_i\\$

==加权平均== $\overline{x} = \frac{\sum_{i=1}^nw_ix_i}{\sum_{i=1}^nw_i}\\$

截尾均值: 去掉高、低极端值得到的均值

- e.g. 计算平均工资时，可以截掉上下各2％的值后计算均值，以抵消少数极端值的影响 ~~(极端数值疑似是有点多了)~~
- **问题**：对极端值很敏感

#### ==中位数==

有序集的中间值或者中间两个值平均 

问题：适用于倾斜数据

#### 众数（Mode，也叫模）

集合中出现频率最高的值，对于适度倾斜（非对称的）的单峰频率曲线，可以 使用以下经验公式计算众数：
$$
mean-mode=3×(mean-median)
$$


**问题**：可能最高频率对应多个不同值，导致多个众数；极端情况下，如果每个 数据值仅出现一次，则它没有众数；

![image-20240412145912662](./数据挖掘.assets/image-20240412145912662.png)

### 2.2.2 离散程度

1. **极差**: 数据集的最大值和最小值之差 
2. **四分位数** 
   - Q1 (25th percentile), Q3 (75th percentile)，中位数就是第2个四分位数(Q2) 
3. **四分位极差（IQR）** 
   - IQR = Q3 – Q1 
   - 通常我们认为，孤立点是指落在第三个四分位数之上或第一个四分位数之下至 少 1.5×IQR 处的值 
4. ==方差和标准差==
   - $\sigma^2=\frac1N\sum_{i=1}^n(x_i-\mu)^2=\frac1N\sum_{i=1}^nx_i^2-\mu^2\\$



## 2.3 数据可视化

略

## 2.4 度量数据的相似性和相异性

> 相似性和相异性都称==邻近性==

1. 相似性 
   - 量化两组数据的相似性
   - 物体相似性越大时，值越大
   - 取值范围是[0,1] 
2. 相异性
   - 量化两组数据的不同的程度
   - 物体相似性越大时，值越小
   - 最小的差异值取0
   - 上限值根据实际不同而不同

### 2.4.2 数据矩阵和相异性矩阵

1. 数据矩阵 

   -  对象-属性结构 
   - 二模矩阵，行与列代表不同实体 
   - 用n×p（n个对象×p个属性）矩阵存放n个对象 
   - $\begin{bmatrix}x_{11}&...&x_{1f}&...&x_{1p}\\...&...&...&...&...\\x_{i1}&...&x_{if}&...&x_{ip}\\...&...&...&...&...\\x_{n1}&...&x_{nf}&...&x_{np}\end{bmatrix}$

2. 相异性矩阵 

   - 对象-对象结构 
   - 单模矩阵，行和列代表相同的实体 
   - 存储n个对象两两之间的相异性

   - $\begin{bmatrix}0&&&&\\d(2,1)&0&&&\\d(3,1)&d(3,2)&\theta&&\\\vdots&\vdots&\vdots&&\\d(n,1)&d(n,2)&...&...&0\end{bmatrix}$

### 2.4.3 临近性度量计算

1. **标称属性**的邻近性度量
   - 对象 i, j 有p个属性，m指匹配的数目，即对象 i 和 j 取相同属性值的数目，
     则对象 i和 j之间的相异性为$d(i,j) = \frac{p-m}{p}$
2. **二元属性**的邻近性度量
   - 二元属性只有两种状态：0或1，0表示该属性不出现，1表示该属性出现
   - 二元属性的列联表 $\begin{array}{cc|ccc}&&\mathbf{Object~j}\\&&1&0&sum\\\hline&1&a&b&a+b\\\mathbf{Object~i}&0&c&d&c+d\\&sum&a+c&b+d&p\end{array}$ 
   - 对称的二元属性，每个状态都同样重要。对象i和j的相异性为： $d(i, j)=\frac{b+c}{a+b+c+d}\\$
   - 非对称的二元属性，两个状态不是同等重要的。对象i与j的相异性为：$d(i,j)=\frac{b+c}{a+b+c}\\$ 
   - 

# DM4 关联规则

## 4.1 关联规则基本概念

1. ==关联规则==(Association Rule)可以表示为一个蕴含式： $R：X\Rightarrow Y$ 
   - 例如：$R：尿布\Rightarrow啤酒$

2. **项集和频繁项集**
   - 项目的集合称为==项集==，项目个数为K的项集称为==K项集==
     例如，{牛奶，面包，黄油}是个3项集
   - 支持度大于或等于**最小支持度**的项集称为==频繁项集==，反之则称为非频繁项集。
   - 如果k项集 满足最小支持度，称为==频繁k项集==，记作$L_k$。

3. ==支持度==

   - 关联规则R的支持度(support)是交易集中同时包含X和Y的交易数与**所有交易数**之比。
   - $support(X\Rightarrow Y) = \frac{count(X\cup Y)}{D} = P(X\cup Y)$
   - P: 概率

4. ==置信度==

   - 关联规则R的置信度(confidence)是指同时包含X和Y的交易数与**包含X的交易数**之比
   - $confidence(X\Rightarrow Y) = \frac{count(X\cup Y)}{count(x)} = \frac{P(X\cup Y)}{P(X)} = P(Y|X)$

5. 关联规则的==最小支持度==和==最小置信度==

   - 关联规则的最小支持度也就是衡量频繁集的最小支持度(Minimum Support)，记为 supmin，它用于衡量规则需要满足的最低重要性。 

   - 关联规则的最小置信度(Minimum Confidence)记为confmin，它用于衡量规则需要满足 的最低可靠性。 

6. ==强关联规则==

   - 如果规则$R:X\Rightarrow Y$满足$\begin{cases}support(X\Rightarrow Y)\geq supmin\  \\ confidence(X\Rightarrow Y)\geq confmin\end{cases}$，称关联规则$X\Rightarrow Y$为强关联规则，否则称关联规则$X\Rightarrow Y$为弱关联规则。 
   - 在挖掘关联规则时，产生的关联规则要经过supmin和confmin衡量，筛选出的强关联规 则才能用于指导商家的决策。



## 4.3 Apriori算法

### 4.3.1 算法描述

Apriori算法由连接和剪枝两个步骤组成

1. 连接：为了找$L_k$ ，通过$L_{k-1}$与自己连接产生候选k项集的集合，该候选k项集记为$C_k$ 。 
   - $L_{k-1}$中的两个元素$l_1$和$l_2$可以执行连接操作$l_1 l_2$的条件是
     $(l_1[1]=l_2[1])\and (l_1[2]=l_2[2])\and ...\and (l_1[k-2]=l_2[k-2]) \and \textcolor{red}{(l_1[k-1]<l_2[k-1])}$
   - $Ck=L_{k-1}\connect L_{k-1}$ 
2. 剪枝：对候选k项集Ck，删除其非频繁的选项，得到Lk
   1. Ck是Lk的超集（Lk Ck ），即它的成员可能不是频繁的，但是所有频繁的都在Ck中。 因此可以通过扫描数据库，侯选项集的计数值不小于最小支持度计数就是频繁的，属 于Lk ； 
   2. 为了减少计算量，可以使用Apriori性质，即如果k项集的一个(k-1)子集不在Lk-1中， 则该候选不可能是频繁的，可以直接从Ck删除。



### 4.3.2 Apriori算法调优

Apriori算法的主要挑战 

- 可能需要重复扫描数据库： 如果频繁集最多包含10个项，那么就需要扫描交易数据表10遍，这需要很大的I/O负载； 
- 可能产生大量的候选项集： 若有100个项目，可能产生候选项集数： 
- 对候选项集的支持度计算非常繁琐； 

解决思路 

- 减少对数据的扫描次数； 
- 缩小产生的候选项集； 
- 改进对候选项集的支持度计算方法

#### ① 散列





# DM5 决策树

## 5.2 决策树归纳

### **信息增益**

信息论中定义事件的平均信息量为单个事件的信息量的统计平均值，称为==期望信息（信息熵）==为$\color{blue}Info(D)=-\sum_{\\i=1}^nP_ilogP_i$ 对所有样本(D)进行分类

$Info_A(D)=\sum_{j=1\\}^{V} \frac{|D_j|}{|D|}Info(D_j)$ 对根据A进行分类后的样本(D~j~)进行分类

$Gain(A)=Info(D)-Info_A(D)$

选择具有最高信息增益的属性进行分裂

### 增益率

> 克服信息增益倾向于选择大量

分裂信息: $\color{blue}SplitInfo_A(D)=-\sum_{j=1\\}^V\frac{|D_j|}{|D|}Info(\frac{|D_j|}{|D|})$

增益率: $GainRate(A)=\frac{Gain(A)}{SplitInfo_A(D)}$ 

### 基尼指数

> 对于二元的划分

$\color{blue}Gini(D)=(\sum_{i=1}^m P_i(1-P_i))=1-\sum_{\\i=1}^mP_i^2$

$Gini_A(D)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$

不纯度降低=$\Delta Gini(A)=Gini(D)-Gini_A(D)$



## 5.3 贝叶斯分类

### 5.3.1 理论基础

设X是样本，用属性值描述，类标号未知； 

设H为某种假设 ，如样本X属于某个特定类C； 

P(H|X)是**后验概率**，或在条件X下，H的后验概率 

- 例如，已经顾客的年龄为35岁且收入为四万美元，该顾客将购买计算机的概率。

P(H)(prior probability)是**先验概率**，或H的先验概率

- 例如，顾客将购买计算机的概率, 无论年龄和收入。 

P(X)是X的**先验概率**，可观察到样本数据

- 例如，顾客集合中年龄为35岁且收入为四万美元的概率。 

P(X|H)是**后验概率**，或在条件H下，X的后验概率

- 例如，已知顾客将购买计算机，该顾客年龄为35岁且收入为四万美元的概率。 



==贝叶斯定理==： $P(H|X)=\frac{P(X|H)P(H)}{P(X)}$

### 5.3.2 朴素贝叶斯分类

## 5.4 基于规则的分类



## 5.5 模型评估与选择

### 5.5.1 术语

正样本（P）：感兴趣的主要类的样本。 

负样本（N）：其他样本。 

真正例（True Positive，TP）：被分类器正确分类的正样本。 

真负例（True Negative，TN）：被分类器正确分类的负样本。 

假正例（False Positive，FP）：被错误地标记为正样本的负样本。 

假负例（False Negative，FN）：被错误地标记为负样本的正样本。

混淆矩阵: 

| $ &(分类器)预测的类\\实际的类$ | TRUE      | FALSE     | 合计 |
| ------------------------------ | --------- | --------- | ---- |
| TRUE                           | TP 真正例 | FN 真负例 | $P$  |
| E                              | FP 假正例 | TN 假负例 | $N$  |
| 合计                           | $P’$      | $N'$      |      |

### 5.5.2

**类分布相对平衡**

- 准确率=$(TP+TN)/(P+N) =灵敏性×P/(P+N)+特效性×N/(P+N) $

- 错误率=$(FP+FN)/(P+N) $

**类不平衡问题**：感兴趣的类（正类）是稀少的，即数据集的分布反映负类显著地占多数，而 正类占少数，例如“欺诈检测”“医学分析” 

- 灵敏性(召回率)： 正确识别的正样本的百分比，$灵敏性 = TP/P $
- 特效性：正确识别的负样本的百分比，$特效性 = TN/N$



  

精度: $precision=TP/(TP+FP)＝TP/P'$ 

召回率: $recall=TP/(TP+FN) ＝ TP/P$

## 5.6 提高分类准确率的技术
