1. 简答: 3~4题 20分
   - 规范化
   - 文本分类
   - 熵
   - 算法区别
2. 计算: 80分 参考作业
   1. A, FP, 关联规则
   2. ID3, KNN, ID3、KNN、BAYES
   3. KMeans算法， 距离计算
   4. 回归（相关性）
   5. 文本分类--PageRank



# DM4 关联规则

## 4.1 关联规则基本概念

1. ==关联规则==(Association Rule)可以表示为一个蕴含式： $R：X\Rightarrow Y$ 
   - 例如：$R：尿布\Rightarrow啤酒$

2. **项集和频繁项集**
   - 项目的集合称为==项集==，项目个数为K的项集称为==K项集==
     例如，{牛奶，面包，黄油}是个3项集
   - 支持度大于或等于**最小支持度**的项集称为==频繁项集==，反之则称为非频繁项集。
   - 如果k项集 满足最小支持度，称为==频繁k项集==，记作$L_k$。

3. ==支持度==

   - 关联规则R的支持度(support)是交易集中同时包含X和Y的交易数与**所有交易数**之比。
   - $support(X\Rightarrow Y) = \frac{count(X\cup Y)}{D} = P(X\cup Y)$
   - P: 概率

4. ==置信度==

   - 关联规则R的置信度(confidence)是指同时包含X和Y的交易数与**包含X的交易数**之比
   - $confidence(X\Rightarrow Y) = \frac{count(X\cup Y)}{count(x)} = \frac{P(X\cup Y)}{P(X)} = P(Y|X)$

5. 关联规则的==最小支持度==和==最小置信度==

   - 关联规则的最小支持度也就是衡量频繁集的最小支持度(Minimum Support)，记为 supmin，它用于衡量规则需要满足的最低重要性。 

   - 关联规则的最小置信度(Minimum Confidence)记为confmin，它用于衡量规则需要满足 的最低可靠性。 

6. ==强关联规则==

   - 如果规则$R:X\Rightarrow Y$满足$\begin{cases}support(X\Rightarrow Y)\geq supmin\  \\ confidence(X\Rightarrow Y)\geq confmin\end{cases}$，称关联规则$X\Rightarrow Y$为强关联规则，否则称关联规则$X\Rightarrow Y$为弱关联规则。 
   - 在挖掘关联规则时，产生的关联规则要经过supmin和confmin衡量，筛选出的强关联规 则才能用于指导商家的决策。



## 4.3 Apriori算法

### 4.3.1 算法描述

Apriori算法由连接和剪枝两个步骤组成

1. 连接：为了找$L_k$ ，通过$L_{k-1}$与自己连接产生候选k项集的集合，该候选k项集记为$C_k$ 。 
   - $L_{k-1}$中的两个元素$l_1$和$l_2$可以执行连接操作$l_1 l_2$的条件是
     $(l_1[1]=l_2[1])\and (l_1[2]=l_2[2])\and ...\and (l_1[k-2]=l_2[k-2]) \and \textcolor{red}{(l_1[k-1]<l_2[k-1])}$
   - $Ck=L_{k-1}\connect L_{k-1}$ 
2. 剪枝：对候选k项集Ck，删除其非频繁的选项，得到Lk
   1. Ck是Lk的超集（Lk Ck ），即它的成员可能不是频繁的，但是所有频繁的都在Ck中。 因此可以通过扫描数据库，侯选项集的计数值不小于最小支持度计数就是频繁的，属 于Lk ； 
   2. 为了减少计算量，可以使用Apriori性质，即如果k项集的一个(k-1)子集不在Lk-1中， 则该候选不可能是频繁的，可以直接从Ck删除。



### 4.3.2 Apriori算法调优

Apriori算法的主要挑战 

- 可能需要重复扫描数据库： 如果频繁集最多包含10个项，那么就需要扫描交易数据表10遍，这需要很大的I/O负载； 
- 可能产生大量的候选项集： 若有100个项目，可能产生候选项集数： 
- 对候选项集的支持度计算非常繁琐； 

解决思路 

- 减少对数据的扫描次数； 
- 缩小产生的候选项集； 
- 改进对候选项集的支持度计算方法

#### ① 散列





# DM5 决策树

## 5.2 决策树归纳

### **信息增益**

信息论中定义事件的平均信息量为单个事件的信息量的统计平均值，称为==期望信息（信息熵）==为$\color{blue}Info(D)=-\sum_{\\i=1}^nP_ilogP_i$ 对所有样本(D)进行分类

$Info_A(D)=\sum_{j=1\\}^{V} \frac{|D_j|}{|D|}Info(D_j)$ 对根据A进行分类后的样本(D~j~)进行分类

$Gain(A)=Info(D)-Info_A(D)$

选择具有最高信息增益的属性进行分裂

### 增益率

分裂信息: $\color{blue}SplitInfo_A(D)=-\sum_{j=1\\}^V\frac{|D_j|}{|D|}Info(\frac{|D_j|}{|D|})$

增益率: $GainRate(A)=\frac{Gain(A)}{SplitInfo_A(D)}$ 

### 基尼指数

$\color{blue}Gini(D)=(\sum_{i=1}^m P_i(1-P_i))=1-\sum_{\\i=1}^mP_i^2$

$Gini_A(D)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$

不纯度降低=$\Delta Gini(A)=Gini(D)-Gini_A(D)$



## 5.3 贝叶斯分类

### 5.3.1 理论基础

设X是样本，用属性值描述，类标号未知； 

设H为某种假设 ，如样本X属于某个特定类C； 

P(H|X)是**后验概率**，或在条件X下，H的后验概率 

- 例如，已经顾客的年龄为35岁且收入为四万美元，该顾客将购买计算机的概率。

P(H)(prior probability)是**先验概率**，或H的先验概率

- 例如，顾客将购买计算机的概率, 无论年龄和收入。 

P(X)是X的**先验概率**，可观察到样本数据

- 例如，顾客集合中年龄为35岁且收入为四万美元的概率。 

P(X|H)是**后验概率**，或在条件H下，X的后验概率

- 例如，已知顾客将购买计算机，该顾客年龄为35岁且收入为四万美元的概率。 



==贝叶斯定理==： $P(H|X)=\frac{P(X|H)P(H)}{P(X)}$

### 5.3.2 朴素贝叶斯分类

## 5.4 基于规则的分类



## 5.5 模型评估与选择

### 5.5.1 术语

正样本（P）：感兴趣的主要类的样本。 

负样本（N）：其他样本。 

真正例（True Positive，TP）：被分类器正确分类的正样本。 

真负例（True Negative，TN）：被分类器正确分类的负样本。 

假正例（False Positive，FP）：被错误地标记为正样本的负样本。 

假负例（False Negative，FN）：被错误地标记为负样本的正样本。

混淆矩阵: 

| $ &(分类器)预测的类\\实际的类$ | TRUE      | FALSE     | 合计 |
| ------------------------------ | --------- | --------- | ---- |
| TRUE                           | TP 真正例 | FN 真负例 | $P$  |
| E                              | FP 假正例 | TN 假负例 | $N$  |
| 合计                           | $P’$      | $N'$      |      |

### 5.5.2

**类分布相对平衡**

- 准确率=$(TP+TN)/(P+N) =灵敏性×P/(P+N)+特效性×N/(P+N) $

- 错误率=$(FP+FN)/(P+N) $

**类不平衡问题**：感兴趣的类（正类）是稀少的，即数据集的分布反映负类显著地占多数，而 正类占少数，例如“欺诈检测”“医学分析” 

- 灵敏性(召回率)： 正确识别的正样本的百分比，$灵敏性 = TP/P $
- 特效性：正确识别的负样本的百分比，$特效性 = TN/N$



  

精度: $precision=TP/(TP+FP)＝TP/P'$ 

召回率: $recall=TP/(TP+FN) ＝ TP/P$

## 5.6 提高分类准确率的技术
