> **课程主页**: https://web.stanford.edu/class/cs224w
> **笔记原文**: https://archives.leni.sh/stanford/CS224w.pdf or [CS224w Notes](./CS224w.pdf)
# 0 Introduction
## 0.1 为什么选择图？

图是一种用于描述和分析具有关系和交互作用的实体的通用语言。其应用场景包括：

- 分子：顶点是原子，边是化学键
- 事件图
- 计算机网络
- 疾病传播路径
- 代码依赖图

复杂领域具有丰富的结构化关系，可以通过关系图来表示。通过显式地建模这些关系，我们可以在更低的模型容量下实现更好的性能。现代机器学习工具处理的是张量，例如图像（二维）、文本/语音（一维）。现代深度学习工具是为简单的序列和网格设计的，并非所有事物都可以表示为序列或网格。那么，我们如何开发出更广泛适用的神经网络呢？答案是使用图，图可以连接一切。

- 图神经网络是 ICLR 2022 的第三大热门关键词。
- 图学习也非常困难，因为图的结构复杂且不规则。
- 图学习也与表示学习有关。在某些情况下，可能可以为图中的每个节点学习一个 d 维嵌入(embedding)，使得相似的节点具有更接近的嵌入。

![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250214220013337.png)
*Figure0.1 当机器学习模型应用于图时, 每个节点都定义了自己的计算图* 

可以针对图数据执行多种任务：

- **节点级预测**：用于表征节点在网络中的结构和位置。例如，在蛋白质折叠中，每个原子是一个节点，任务是预测节点的坐标。
- **边/链接级预测**：预测一对节点的属性。这可以是寻找缺失的链接，也可以是随着时间推移发现新的链接。例如，基于图的推荐系统和药物副作用预测。
- **图级预测**：对整个子图或图进行预测。例如，交通预测、药物发现、物理模拟和天气预报。
![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250214220755253.png)
*不同级别的图任务*

## 0.2 图嵌入的选择

图包含以下几种组成部分：
- 对象 N：节点Nodes、顶点vertices
- 交互 E：边Edges、链接Links
- 系统 G(N, E)：网络Networks、图graphs

在某些情况下，图有统一的表示形式，而在某些情况下则没有。表示形式的选择决定了可以从图中提取哪些信息。图还可能具有其他属性：

- 无向/有向边
- 允许/不允许自环
- 允许/不允许多图（节点之间有多条边）
- **异构图heterogeneous graph**：图 $G = (V, E, T, R, τ, ϕ)$，其中节点有类型 $τ(v) : T$，边有类型 $ϕ(e) : R$。
	- 许多图是异构的。例如，药物-蛋白质相互作用图是异构的。
- **二分图**：例如作者-论文图、演员-电影图

大多数真实世界的网络是稀疏的。邻接矩阵是一个稀疏矩阵，其中大部分是 0。矩阵的密度 (E/N²) 对于 WWW 是 1.51×10⁻⁵，对于 MSN IM 是 2.27×10⁻⁸。


# 1 传统机器学习中的图

在传统的机器学习流程中，例如逻辑回归、随机森林和神经网络，模型首先在图的特征上进行训练，然后可以应用于新的图。**使用有效的图特征是实现良好模型性能的关键**。为了简化，本节我们关注无向图。

## 1.1 节点级特征

节点级任务的一个简单例子是基于少量样本的节点分类(如图所示)。
![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250214220859151.png)*在仅提供较少标签(样本)的情况下, 对图节点进行分类*

几种不同的度量可以表征节点在图中的结构和位置：节点度, [节点中心度](#节点中心度), [聚类系数](#聚类系数与图元Graphlet)

**节点度**：邻居的数量。
### 节点中心度

**节点中心性**：衡量节点在图中“重要性”的指标。存在几种不同类型的中心性。
- **特征向量中心性**：如果一个节点被重要的邻居节点包围，那么这个节点就是重要的。我们定义节点 v 的中心性为邻居节点的中心性。这导致了一组 |N| 的线性方程： $$c_{v}​ := \frac 1λ\sum_{​u∈N(v)}​c_{u}​\Longleftrightarrow λ\mathbf c=\mathbf A\mathbf c$$
	其中，λ 是归一化常数，A 是图的邻接矩阵, $N(v)$ 是节点 v 的所有邻节点集合。根据佩龙-弗罗贝尼乌斯Perron-Frobeniu定理，最大特征值 λmax​ 总是正的且唯一，**其对应的特征向量可用于中心性计算**。当 λ 是第二大的特征值时，$c_{v}$​ 有不同的含义。
	> 太好了, 是线性代数, 已经全忘了
- **介数中心性**：如果一个节点是“把关者”，即它位于许多其他节点之间的最短路径上，那么这个节点就是重要的。 $$c_v:=\sum_{s\neq v\neq t}\frac{\text{包含 }v\text{ 的 }s,t\text{ 之间的最短路径数量}}{\text{所有 }s,t\text{ 之间的最短路径数量}}$$在社交网络中，这一指标非常重要。
- **接近中心性Closeness centrality**： $$c_v:=\frac{1}{\sum_{u\neq v}v,u\text{ 之间的最短路径长度}}$$
### 聚类系数与图元Graphlet

**聚类系数Clustering coefficient**：衡量节点 v 的邻居节点之间的连接紧密程度    $$e_v:=\frac{|\{N(v)\text{之间的边}\}|}{\binom{k_{v}}{2}}\in[0,1]$$
- 注: 组合数$$\binom{N}{k}=C(N,k)=C_{N}^k=\frac{N!}{k!(N-k)!}$$
- 社交网络中存在大量的聚类现象。
- ![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250215003417510.png)
*一些聚类系数的例子*

聚类系数计算了 **ego 网络**（由 $\{v\}∪N(v)$ 形成的网络，其中 v 是 ego）中的三角形数量。![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250215144835846.png)
> 注: **Ego网络（Ego Network）** 是社会网络分析中的一个重要概念，它指的是围绕一个中心节点（称为“ego”）构建的子网络，包括该中心节点及其直接连接的所有邻居节点（称为“alters”），以及这些邻居节点之间的连接。

我们可以将上述内容推广为计算图元（graphlets）。

**图元（Graphlets）** 是描述节点 u 的邻域网络结构的小型**子图**。具体来说，它们是*Rooted、连通的Connected、导出的induced、非同构的non-isomorphic*子图。
- Rooted: 指graphlet需要考虑它的根节点(如3-node graphlets中所示, 链式的有两种graphlets)
- **导出子图/诱导子图induced subgraph**是指, 由较大图顶点的一个子集和该图中两端均在该子集的所有边的集合组成的图。
- 如果两个图具有相同的拓扑结构，则它们是**同构isomorphic**的。
- 对于5个节点, 可以提取出73种graphlet 
  ![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250215174652376.png)


(节点的)**图元度向量（Graphlet Degree Vector, GDV）**: 当我们考虑大小为2~5个节点的图时，我们得到一个包含73个元素（即从2到5个顶点的图元数量）的向量，用于描述节点邻域的拓扑结构。这个向量被称为节点的**图元度向量（Graphlet Degree Vector, GDV）**。
![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250215151735498.png)*GDV例*
- 借助GDV, 可以更精细的比较两个图的局部结构相似性

> 到目前为止我们讨论的特征捕捉了图的局部拓扑属性，但无法在全局范围内区分点。

## 1.2 链接级特征

链接级预测有两种表述方式：
- **随机缺失的链接**：从图中随机移除一组链接，然后尝试预测它们。
    - 例如，药物相互作用。
- **随时间变化的链接**：给定一个由时间$t_{0}'$之前的边定义的图 $G[t_{0},t_{0}']$，**输出一个排名的链接列表 L**，预测在时间 $G[t_{1}​,t_{1}']$ 中可能出现的边。
	- 如已知当前的好友网络, 预测接下来哪些人最有可能加为好友
	- 评估方法：设 $n=∣E_{new}​∣$ 为测试期间($[t_{1}​,t_{1}']$ )新出现的边的数量。
	- 方法：对于每一对节点 x,y，计算一个分数 c(x,y)，并将排名最高的 n 个元素预测为链接。

几种用于局部邻域重叠的度量方法如下：
- **共同邻居**： $c(v_{1},v_{2}):=∣N(v_1​)∩N(v_2​)∣$
- **杰卡德系数Jaccard's coefficient**：$c(u,v):=\frac{∣N(v_1)∪N(v_2​)∣}{∣N(v_1)∩N(v_2​)∣}​$
- **阿达米克-阿达尔指数Adamic-Adar index**： $c(u,v):=\sum_{u∈N(v_1)∩N(v_2​)}\frac{1}{log(k_{u})}$ ![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250215154705555.png)
- 
上述三种指标的问题是，如果 u,v 没有共享邻居，它们的值总是为 0。

## 1.3 图核方法

图核方法的目标是为整个图创建一个特征向量。核方法广泛用于传统机器学习中的图级预测任务。与其设计特征向量，不如设计核函数：

- $k(G,G')∈R$
- 核矩阵 $K=[K(G,G')]_{G,G'}$​ 必须始终是半正定的。
- 存在一个特征表示 ϕ，使得 $K(G,G')=ϕ(G)⋅ϕ(G')$，这甚至可以是无限维的。

我们可以使用“词袋”（Bag-of-Words, BOW）来表示图。回想在自然语言处理中，BOW 仅使用单词计数作为文档的特征，而不考虑顺序。我们将节点视为“单词”。

**图级图元特征 Graph-Level Graphlet features** 计算图中不同图元的数量。这里的图元与节点级特征中的定义略有不同，它们不是根节点，也不需要是连通的。这一定义的局限性在于，计算图元是昂贵的。通过枚举计算大小为 k 的图元在大小为 n 的图中需要 $O(n^k)$ 时间，因为子图同构测试成本高昂。如果图的节点度数有界，则时间可以压缩到 $O(nd^{k−1})$。

到目前为止，我们只考虑了与图结构相关的特征，而没有考虑节点及其邻居的属性。
