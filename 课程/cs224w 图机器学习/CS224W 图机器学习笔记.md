> **课程主页**: https://web.stanford.edu/class/cs224w
> **笔记原文**: https://archives.leni.sh/stanford/CS224w.pdf or [CS224w Notes](./CS224w.pdf)
# 0 Introduction
## 0.1 为什么选择图？

图是一种用于描述和分析具有关系和交互作用的实体的通用语言。其应用场景包括：

- 分子：顶点是原子，边是化学键
- 事件图
- 计算机网络
- 疾病传播路径
- 代码依赖图

复杂领域具有丰富的结构化关系，可以通过关系图来表示。通过显式地建模这些关系，我们可以在更低的模型容量下实现更好的性能。现代机器学习工具处理的是张量，例如图像（二维）、文本/语音（一维）。现代深度学习工具是为简单的序列和网格设计的，并非所有事物都可以表示为序列或网格。那么，我们如何开发出更广泛适用的神经网络呢？答案是使用图，图可以连接一切。

- 图神经网络是 ICLR 2022 的第三大热门关键词。
- 图学习也非常困难，因为图的结构复杂且不规则。
- 图学习也与表示学习有关。在某些情况下，可能可以为图中的每个节点学习一个 d 维嵌入(embedding)，使得相似的节点具有更接近的嵌入。

![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250214220013337.png)
*Figure0.1 当机器学习模型应用于图时, 每个节点都定义了自己的计算图* 

可以针对图数据执行多种任务：

- **节点级预测**：用于表征节点在网络中的结构和位置。例如，在蛋白质折叠中，每个原子是一个节点，任务是预测节点的坐标。
- **边/链接级预测**：预测一对节点的属性。这可以是寻找缺失的链接，也可以是随着时间推移发现新的链接。例如，基于图的推荐系统和药物副作用预测。
- **图级预测**：对整个子图或图进行预测。例如，交通预测、药物发现、物理模拟和天气预报。
![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250214220755253.png)
*不同级别的图任务*

## 0.2 图嵌入的选择

图包含以下几种组成部分：
- 对象 N：节点Nodes、顶点vertices
- 交互 E：边Edges、链接Links
- 系统 G(N, E)：网络Networks、图graphs

在某些情况下，图有统一的表示形式，而在某些情况下则没有。表示形式的选择决定了可以从图中提取哪些信息。图还可能具有其他属性：

- 无向/有向边
- 允许/不允许自环
- 允许/不允许多图（节点之间有多条边）
- **异构图heterogeneous graph**：图 $G = (V, E, T, R, τ, ϕ)$，其中节点有类型 $τ(v) : T$，边有类型 $ϕ(e) : R$。
	- 许多图是异构的。例如，药物-蛋白质相互作用图是异构的。
- **二分图**：例如作者-论文图、演员-电影图

大多数真实世界的网络是稀疏的。邻接矩阵是一个稀疏矩阵，其中大部分是 0。矩阵的密度 (E/N²) 对于 WWW 是 1.51×10⁻⁵，对于 MSN IM 是 2.27×10⁻⁸。


# 1 传统机器学习中的图

在传统的机器学习流程中，例如逻辑回归、随机森林和神经网络，模型首先在图的特征上进行训练，然后可以应用于新的图。**使用有效的图特征是实现良好模型性能的关键** -> 特征工程。

> 为了简化，本节我们关注无向图。

## 1.1 节点级特征

节点级任务的一个简单例子是基于少量样本的节点分类(如图所示)。
![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250214220859151.png)*在仅提供较少标签(样本)的情况下, 对图节点进行分类*

几种不同的度量可以表征节点在图中的结构和位置：节点度, [节点中心度](#节点中心度), [聚类系数](#聚类系数与图元Graphlet)

**节点度**：邻居的数量。
### 节点中心度

**节点中心性**：衡量节点在图中“重要性”的指标。存在几种不同类型的中心性。
- **特征向量中心性**：如果一个节点被重要的邻居节点包围，那么这个节点就是重要的。我们定义节点 v 的中心性为邻居节点的中心性。这导致了一组 |N| 的线性方程： $$c_{v}​ := \frac 1λ\sum_{​u∈N(v)}​c_{u}​\Longleftrightarrow λ\mathbf c=\mathbf A\mathbf c$$
	其中，λ 是归一化常数，A 是图的邻接矩阵, $N(v)$ 是节点 v 的所有邻节点集合。根据佩龙-弗罗贝尼乌斯Perron-Frobeniu定理，最大特征值 λmax​ 总是正的且唯一，**其对应的特征向量可用于中心性计算**。当 λ 是第二大的特征值时，$c_{v}$​ 有不同的含义。
	> 太好了, 是线性代数, 已经全忘了
- **介数中心性**：如果一个节点是“把关者”，即它位于许多其他节点之间的最短路径上，那么这个节点就是重要的。 $$c_v:=\sum_{s\neq v\neq t}\frac{\text{包含 }v\text{ 的 }s,t\text{ 之间的最短路径数量}}{\text{所有 }s,t\text{ 之间的最短路径数量}}$$在社交网络中，这一指标非常重要。
- **接近中心性Closeness centrality**： $$c_v:=\frac{1}{\sum_{u\neq v}v,u\text{ 之间的最短路径长度}}$$
### 聚类系数与图元Graphlet

**聚类系数Clustering coefficient**：衡量节点 v 的邻居节点之间的连接紧密程度    $$e_v:=\frac{|\{N(v)\text{之间的边}\}|}{\binom{k_{v}}{2}}\in[0,1]$$
- 注: 组合数$$\binom{N}{k}=C(N,k)=C_{N}^k=\frac{N!}{k!(N-k)!}$$
- 社交网络中存在大量的聚类现象。
- ![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250215003417510.png)
*一些聚类系数的例子*

聚类系数计算了 **ego 网络**（由 $\{v\}∪N(v)$ 形成的网络，其中 v 是 ego）中的三角形数量。![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250215144835846.png)
> 注: **Ego网络（Ego Network）** 是社会网络分析中的一个重要概念，它指的是围绕一个中心节点（称为“ego”）构建的子网络，包括该中心节点及其直接连接的所有邻居节点（称为“alters”），以及这些邻居节点之间的连接。

我们可以将上述内容推广为计算图元（graphlets）。

**图元（Graphlets）** 是描述节点 u 的邻域网络结构的小型**子图**。具体来说，它们是*Rooted、连通的Connected、导出的induced、非同构的non-isomorphic*子图。
- Rooted: 指graphlet需要考虑它的根节点(如3-node graphlets中所示, 链式的有两种graphlets)
- **导出子图/诱导子图induced subgraph**是指, 由较大图顶点的一个子集和该图中两端均在该子集的所有边的集合组成的图。
- 如果两个图具有相同的拓扑结构，则它们是**同构isomorphic**的。
- 对于5个节点, 可以提取出73种graphlet 
  ![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250215174652376.png)


(节点的)**图元度向量（Graphlet Degree Vector, GDV）**: 当我们考虑大小为2~5个节点的图元时，我们得到一个包含73个元素（即从2到5个顶点的图元数量）的向量，用于描述节点邻域的拓扑结构。这个向量被称为节点的**图元度向量（Graphlet Degree Vector, GDV）**。
![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250215151735498.png)*GDV例*
- 借助GDV, 可以更精细的比较两个图的局部结构相似性

> 到目前为止我们讨论的特征捕捉了图的局部拓扑属性，但无法在全局范围内区分点。

## 1.2 链接级特征

链接级预测有两种表述方式：
- **随机缺失的链接**：从图中随机移除一组链接，然后尝试预测它们。
    - 例如，药物相互作用。
- **随时间变化的链接**：给定一个由时间$t_{0}'$之前(时间段)的边定义的图 $G[t_{0},t_{0}']$，**输出一个排名的链接列表 L**，预测在时间 $G[t_{1}​,t_{1}']$ 中可能出现的边。
	- 如已知当前的好友网络, 预测接下来哪些人最有可能加为好友
	- 评估方法：设 $n=∣E_{new}​∣$ 为测试期间($[t_{1}​,t_{1}']$ )新出现的边的数量。
	- 方法：对于每一对节点 x,y，计算一个分数 c(x,y)，并将排名最高的 n 个元素预测为链接。

几种用于局部邻域重叠的度量方法如下：
- **共同邻居**： $$c(v_{1},v_{2}):=∣N(v_1​)∩N(v_2​)∣$$
- **杰卡德系数Jaccard's coefficient**：$$c(u,v):=\frac{∣N(v_1)∪N(v_2​)∣}{∣N(v_1)∩N(v_2​)∣}​$$
- **阿达米克-阿达尔指数Adamic-Adar index**： $$c(u,v):=\sum_{u∈N(v_1)∩N(v_2​)}\frac{1}{log(k_{u})}$$ ![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250215154705555.png)
- 
上述三种指标的问题是，如果 u,v 没有共享邻居，它们的值总是为 0。

- **Katz 指数**：$$ c(u, v) := | \{\text{节点 } u \text{ 和 } v \text{ 之间所有长度的所有路径}\} |$$
	- 这可以通过邻接矩阵的幂来计算。计算顶点之间长度为n的所有路径的矩阵是 $\mathbf A^n$，因此 Katz 指数可以通过以下公式计算：$$\mathbf C := \sum_{i=1}^{\infty} \beta^i \mathbf A^i = (I - \beta \mathbf A)^{-1} - \mathbf I $$其中，$\beta < 1$ 是一个衰减因子，用于防止 C 发散至正无穷大。
	- 对于有向图也有类似的定义。

### 解释

1. **定义**：
   - \( c(u, v) \) 表示节点 \( u \) 和节点 \( v \) 之间所有长度的所有路径的数量。
   
2. **计算方法**：
   - 使用邻接矩阵 \( A \) 的幂次来计算路径数量。具体来说，\( A^n \) 矩阵中的元素 \( (A^n)_{uv} \) 表示从节点 \( u \) 到节点 \( v \) 长度为 \( n \) 的路径数目。
   - 所有长度路径的总和可以通过无限级数 \( \sum_{i=1}^{\infty} \beta^i A^i \) 来表示，其中 \( \beta \) 是一个衰减因子。

3. **衰减因子 \( \beta \)**：
   - \( \beta \) 必须小于 1，以确保上述级数收敛，从而防止 \( C \) 发散到正无穷大。衰减因子的作用是给较长路径赋予较小的权重，因为较长路径通常不如较短路径重要。
   
4. **公式推导**：
   - 上述级数可以简化为矩阵形式：\( C = (I - \beta A)^{-1} - I \)，其中 \( I \) 是单位矩阵。
   - 这个公式通过矩阵求逆操作计算了所有长度路径的加权和，减去单位矩阵 \( I \) 是为了去除路径长度为零的情况（即节点与自身的直接连接）。

5. **有向图的应用**：
   - 类似的定义也适用于有向图。在这种情况下，邻接矩阵 \( A \) 中的元素 \( A_{ij} \) 表示从节点 \( i \) 到节点 \( j \) 是否存在一条有向边。其余计算过程与无向图相同。



## 1.3 图核方法

图核方法的目标是为**整个图创建一个特征向量**。核方法广泛用于传统机器学习中的图级预测任务。与其设计特征向量，不如设计核函数：
- $K(G,G')∈R$
- 核矩阵 $K=[K(G,G')]_{G,G'}$​ 必须始终是半正定的。
- 存在一个特征表示 ϕ，使得 $K(G,G')=ϕ(G)⋅ϕ(G')$，这甚至可以是无限维的。

我们可以使用“词袋”（Bag-of-Words, BOW）来表示图。回想在自然语言处理中，BOW 仅使用单词计数作为文档的特征向量，而不考虑顺序。我们将节点视为“单词”。

**图级图元特征 Graph-Level Graphlet features** 计算图中不同图元的数量。这里的图元与节点级特征中的定义略有不同，它们不是根节点，也不需要是连通的。这一定义的局限性在于，计算图元是昂贵的。通过枚举计算大小为 k 的图元在大小为 n 的图中需要 $O(n^k)$ 时间，因为子图同构测试成本高昂。如果图的节点度数有界，则时间可以压缩到 $O(nd^{k−1})$。

到目前为止，我们只考虑了与图结构相关的特征，而没有考虑节点及其邻居的属性。

# 2 节点嵌入 (Node Embeddings)

表示学习避免了每次都需要进行特征工程。目标是将单个节点或整个图映射到向量中，即**嵌入embedding**。在节点嵌入中，我们希望嵌入具有以下属性：
- 嵌入之间的相似性表示节点在网络中的相似性
  - 例如，距离更近的节点可以被认为是相似的
- 编码网络信息
- 对下游任务可能有用

假设我们有一个图G，我们希望为G的节点V设计一个嵌入。目标是编码节点，使得嵌入空间中的相似性近似于图中的相似性。我们有3个设计选择：编码器、解码器和目标相似性函数。
- 编码器：$Enc: V →  R^{(d×|V|)}，Enc(u) := \mathbf z_u$。这可以被定义为从节点到向量的简单查找表。在这种情况下，它是一个浅层编码器。使用GNN的深层编码器将在第5.1节中介绍。这是唯一的可学习函数。
- 解码器：给定两个嵌入，测量相似性。通常选择为点积 $Dec(z, w) = z·w$。
- 相似性：节点之间的相似性度量。这可以是图中的距离、邻域拓扑等。它由编码器和解码器的组合近似。$similarity(u, v) ≃ Dec(Enc(u), Enc(v))$。
	- 在本例中，$similarity(u, v) ≃ \mathbf z_u·\mathbf z_v$。

许多方法都源于这种简单的设置，包括DeepWalk和node2vec。

## 2.1 随机游走嵌入 (Random Walk Embedding)

基于随机游走的无监督/自监督节点相似性度量在这种情况下，我们不使用节点标签或特征。嵌入是任务无关的，并且不使用任何节点标签或特征。选择随机游走的理由如下：
- 表达性：灵活的随机定义的节点相似性，结合了低阶和高阶邻域信息。
- 效率：在训练时不需要完全遍历图。

我们定义：
- 向量$Enc(u) := z_u$：节点u的嵌入。
- 概率$P(v|z_u)$：从u开始的随机游走中访问v的预测概率。
- Softmax函数：将k个值的向量转换为总和为1的k个概率：  $$σ(z) = exp(z) / ∑_{(k=1)}^K exp(z_k)$$
- Sigmoid函数：将R压缩到(0, 1)：
  $S(x) = 1 / (1 + exp(-x))$

- 随机游走是一系列随机顶点$v_0, v_1, ...$，使得$v_{(i+1)}$是从$N(v_i)$中随机选择的。
- 目标相似性函数为：  $$similarity(u, v) := P(u, v出现在G上的随机游走中)$$
- 我们选择一个随机游走策略R，并使用这种策略来确定P_R(v|u)，即从u开始的随机游走访问v的概率。策略定义了N_R(u)，这是一个从u开始的所有短固定长度随机游走收集的随机多重集（可以有重复，因为节点可以多次访问，本质上是一个概率分布）。

现在我们可以用数学方式表述目标函数。负对数似然为：
$$L(Enc) := -∑_{(u∈V)} log P(N_R(u)|z_u) = -∑_{(u∈V)} ∑_{(v∈N_R(u))} log P(v|z_u)$$

对所有顶点求和
对从u开始的随机游走中看到的节点求和
预测u和v共现的概率

我们使用softmax参数化P(v|z_u)。本质上，我们将指数化的相似性分数exp(z_u·z_v)视为未归一化的概率：
P(v|z_u) := exp(z_u·z_v) / ∑_(n∈V) exp(z_u·z_n)

然而，这个函数的计算成本很高。两个∑_(n∈V)循环已经给出了O(|V|^2)的时间复杂度。解决这个问题的方法是负采样，它提供了估计：
log(exp(z_u·z_v) / ∑_(n∈V) exp(z_u·z_n)) ≃ log σ(z_u·z_v) - ∑_(i=1)^k log σ(z_u·z_n_i) (n_i ∼ P_v)

其中P_v是V上的概率分布。而不是相对于所有节点进行归一化，只需相对于k个随机负样本n_i进行归一化。我们可以选择P_V，使得P_V(n) ∝ deg n。负样本的数量k通常选择为5到20，因为：
- 较高的k值可以提供更稳健的估计。
- 较高的k值对应于负事件的更高偏差。

有了这些，我们可以稳健地评估∂L/∂z_u，并执行随机梯度下降（SGD）以优化Enc。

1. 这是一种噪声对比估计（NCE）的形式。参见https://arxiv.org/pdf/1402.3722.pdf

---

### 随机梯度下降的回顾

在随机游走嵌入的设置中回顾随机梯度下降算法：
1. 随机初始化所有u的z_u。
2. 对所有u：
   - 计算∂L/∂z_u
   - 执行一步更新 z_u ← z_u - η·∂L/∂z_u。η是学习率。
3. 返回步骤(2)，直到收敛。

---

对于随机游走策略R，有几种不同的选择。在DeepWalk中，这只是从每个节点开始的无偏随机游走，但这可能过于受限。在node2vec中，策略被选择为将具有相似网络邻域的节点嵌入到特征空间中更接近的位置。我们将这视为一个最大似然优化问题，该问题独立于下游预测任务。关键观察是灵活的N_R定义可以产生丰富的节点嵌入。

两种经典的邻域定义策略是：广度优先搜索（BFS）和深度优先搜索（DFS）。我们可以通过两个参数在这两者之间进行插值：
- 返回参数p：返回到前一个节点的概率。
- 进出参数q：向外（DFS）与向内（BFS）移动的概率。直观上，q是插值参数。

我们定义一个有偏的二阶随机游走来探索网络邻域。假设我们刚刚沿着边(s_1, w)旅行，现在处于w。在N(w)中前方有三个选择。
- 返回到s_1（距离0），权重为1/p
- 保持在N(s_1)中（距离1），每个节点的权重为1
- 离开N(s_1)并进一步探索（距离2），每个更远的节点的权重为1/q

---

BFS类游走具有较低的p值（易于回溯），而DFS类游走具有较低的q值。在2017年的一项调查中，node2vec在节点分类任务中表现更好，而其他方法在链接预测任务中表现更好。没有一种方法能在所有情况下获胜。随机游走方法通常更高效。

---

## 2.2 嵌入整个图 (Embedding Entire Graphs)

有时我们希望将整个图嵌入到某个嵌入空间中。这可以用于例如对分子进行有毒/无毒分类或识别异常图。一种简单的方法是对现有节点嵌入求和：
z_G := ∑_(v∈G) z_v

另一种方法是引入一个“虚拟节点”来代表子图，并运行节点嵌入算法以使用其嵌入。我们将讨论层次嵌入，它通过逐步总结图来生成嵌入。

---

## 2.3 与矩阵分解的关系 (Relations to Matrix Factorization)

回想一下，编码器可以被视为一个嵌入查找表，大小为d×|V|。最简单的节点相似性是邻接矩阵。当两个节点通过边连接时，它们被认为是相似的，否则不相似。这就是邻接矩阵A，尝试学习嵌入是一种矩阵分解A = Z^TZ。由于d ≪ |V|，这种分解无法精确完成，因为存在秩问题，因此我们可能需要优化以最小化Frobenius范数 min_Z ||A - Z^TZ||_F。在DeepWalk的例子中，嵌入分解矩阵为
log(vol(G) / (1/T ∑_(t=1)^T (D^(-1)A)^t D^(-1)) - log b)
其中：
- 图的体积 vol(G) := ∑_(i,j) A_i,j
- 上下文窗口大小 T := |N_R(u)|
- 度矩阵 D_u,u := deg u
- 负样本数量

本质上，D^(-1)A是随机游走的马尔可夫矩阵。

---

## 2.4 应用与局限性 (Applications and Limitations)

嵌入可用于以下场景：
- 聚类/社区检测
- 节点分类
- 链接预测：基于(zi, zj)预测边(i, j)
- 图分类：对zG进行分类

通过矩阵分解和随机游走进行节点嵌入存在一些局限性：
- 需要O(|V|·d)个参数。
  - 节点之间无法共享参数，每个节点都有其独特的嵌入。
- 传导性：只有在看到图中的所有节点后，才能生成嵌入。无法为训练集中未出现的节点获得嵌入。
- 无法捕获局部拓扑的结构相似性。
- 无法利用节点、边和图的特征。

深度表示学习和图神经网络缓解了这些局限性，这将在第4节和第5节中详细介绍。