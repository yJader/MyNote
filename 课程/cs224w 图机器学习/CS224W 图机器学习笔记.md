> **笔记原文**: https://archives.leni.sh/stanford/CS224w.pdf or [CS224w Notes](./CS224w.pdf)
# 0 Introduction
## 0.1 为什么选择图？

图是一种用于描述和分析具有关系和交互作用的实体的通用语言。其应用场景包括：

- 分子：顶点是原子，边是化学键
- 事件图
- 计算机网络
- 疾病传播路径
- 代码依赖图

复杂领域具有丰富的结构化关系，可以通过关系图来表示。通过显式地建模这些关系，我们可以在更低的模型容量下实现更好的性能。现代机器学习工具处理的是张量，例如图像（二维）、文本/语音（一维）。现代深度学习工具是为简单的序列和网格设计的，并非所有事物都可以表示为序列或网格。那么，我们如何开发出更广泛适用的神经网络呢？答案是使用图，图可以连接一切。

- 图神经网络是 ICLR 2022 的第三大热门关键词。
- 图学习也非常困难，因为图的结构复杂且不规则。
- 图学习也与表示学习有关。在某些情况下，可能可以为图中的每个节点学习一个 d 维嵌入(embedding)，使得相似的节点具有更接近的嵌入。

![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250214220013337.png)
*Figure0.1 当机器学习模型应用于图时, 每个节点都定义了自己的计算图* 

可以针对图数据执行多种任务：

- **节点级预测**：用于表征节点在网络中的结构和位置。例如，在蛋白质折叠中，每个原子是一个节点，任务是预测节点的坐标。
- **边/链接级预测**：预测一对节点的属性。这可以是寻找缺失的链接，也可以是随着时间推移发现新的链接。例如，基于图的推荐系统和药物副作用预测。
- **图级预测**：对整个子图或图进行预测。例如，交通预测、药物发现、物理模拟和天气预报。
![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250214220755253.png)
*不同级别的图任务*

## 0.2 图嵌入的选择

图包含以下几种组成部分：
- 对象 N：节点Nodes、顶点vertices
- 交互 E：边Edges、链接Links
- 系统 G(N, E)：网络Networks、图graphs

在某些情况下，图有统一的表示形式，而在某些情况下则没有。表示形式的选择决定了可以从图中提取哪些信息。图还可能具有其他属性：

- 无向/有向边
- 允许/不允许自环
- 允许/不允许多图（节点之间有多条边）
- **异构图heterogeneous graph**：图 $G = (V, E, T, R, τ, ϕ)$，其中节点有类型 $τ(v) : T$，边有类型 $ϕ(e) : R$。
	- 许多图是异构的。例如，药物-蛋白质相互作用图是异构的。
- **二分图**：例如作者-论文图、演员-电影图

大多数真实世界的网络是稀疏的。邻接矩阵是一个稀疏矩阵，其中大部分是 0。矩阵的密度 (E/N²) 对于 WWW 是 1.51×10⁻⁵，对于 MSN IM 是 2.27×10⁻⁸。


# 1 传统机器学习中的图

在传统的机器学习流程中，例如逻辑回归、随机森林和神经网络，模型首先在图的特征上进行训练，然后可以应用于新的图。**使用有效的图特征是实现良好模型性能的关键**。为了简化，本节我们关注无向图。

## 1.1 节点级特征

节点级任务的一个简单例子是基于少量样本的节点分类(如图所示)。
![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250214220859151.png)*在仅提供较少标签(样本)的情况下, 对图节点进行分类*

几种不同的度量可以表征节点在图中的结构和位置：节点度, [节点中心度](#节点中心度), [聚类系数](#聚类系数)

**节点度**：邻居的数量。
### 节点中心度

**节点中心性**：衡量节点在图中“重要性”的指标。存在几种不同类型的中心性。
- **特征向量中心性**：如果一个节点被重要的邻居节点包围，那么这个节点就是重要的。我们定义节点 v 的中心性为邻居节点的中心性。这导致了一组 |N| 的线性方程： $$c_{v}​ := \frac 1λ\sum_{​u∈N(v)}​c_{u}​\Longleftrightarrow λ\mathbf c=\mathbf A\mathbf c$$
	其中，λ 是归一化常数，A 是图的邻接矩阵, $N(v)$ 是节点 v 的所有邻节点集合。根据佩龙-弗罗贝尼乌斯Perron-Frobeniu定理，最大特征值 λmax​ 总是正的且唯一，**其对应的特征向量可用于中心性计算**。当 λ 是第二大的特征值时，$c_{v}$​ 有不同的含义。
	> 太好了, 是线性代数, 已经全忘了
- **介数中心性**：如果一个节点是“把关者”，即它位于许多其他节点之间的最短路径上，那么这个节点就是重要的。 $$c_v:=\sum_{s\neq v\neq t}\frac{\text{包含 }v\text{ 的 }s,t\text{ 之间的最短路径数量}}{\text{所有 }s,t\text{ 之间的最短路径数量}}$$在社交网络中，这一指标非常重要。
- **接近中心性Closeness centrality**： $$c_v:=\frac{1}{\sum_{u\neq v}v,u\text{ 之间的最短路径长度}}$$
### 聚类系数

**聚类系数Clustering coefficient**：衡量节点 v 的邻居节点之间的连接紧密程度    $$e_v:=\frac{|\{N(v)\text{之间的边}\}|}{\binom{k_{v}}{2}}\in[0,1]$$
- 注: $$\binom{N}{k}=C(N,k)=C_{N}^k=\frac{N!}{k!(N-k)!}$$
- 社交网络中存在大量的聚类现象。
- ![](CS224W%20图机器学习笔记.assets/IMG-CS224W%20图机器学习笔记-20250215003417510.png)*一些聚类系数的例子*

聚类系数计算了 ego 网络（由 $\{v\}∪N(v)$ 形成的网络，其中 v 是 ego）中的三角形数量。我们可以将上述内容推广为计算图元（graphlets）。

**诱导子图induced graph**是由较大图的一个顶点子集形成的图，其中仅保留剩余顶点之间的边。
如果两个图具有相同的拓扑结构，则它们是**同构isomorphic**的。

**图元（Graphlets）** 是描述节点 u 的邻域网络结构的小型子图。具体来说，它们是*根植的、连通的、导出的、非同构的*子图。当我们考虑大小为2~5个节点的图时，我们得到一个包含73个元素（即从2到5个顶点的图元数量）的向量，用于描述节点邻域的拓扑结构。这个向量被称为节点的**图元度向量（Graphlet Degree Vector, GDV）**。

到目前为止我们讨论的特征捕捉了图的局部拓扑属性，但无法在全局范围内区分点。

## 1.2 链接级特征

链接级预测有两种表述方式：

- **随机缺失的链接**：从图中随机移除一组链接，然后尝试预测它们。
    - 例如，药物相互作用。
        
- **随时间变化的链接**：给定一个定义到时间 t′ 的图 G[t0​,t′]，输出一个排名列表 L，预测在时间 G[t1​,t′] 中可能出现的边。
    - 评估方法：设 n=∣Enew​∣ 为测试期间新出现的边的数量。
    - 方法：对于每一对节点 x,y，计算一个距离 c(x,y)，并将排名最高的 n 个元素预测为链接。
        

几种用于局部邻域重叠的度量方法如下：

- **共同邻居**： 
    
    c(u,v):=∣N(v1​)∩N(v2​)∣
- **杰卡德系数**： 
    
    c(u,v):=∣N(v1​)∪N(v2​)∣∣N(v1​)∩N(v2​)∣​
- **阿达米克-阿达尔指数**： 
    
    c(u,v):=u∈N(v1​)∩N(v2​)∑​ku​1​

上述三种指标的问题是，如果 u,v 没有共享邻居，它们的值总是为 0。

---

## 1.3 图核方法

图核方法的目标是为整个图创建一个特征向量。核方法广泛用于传统机器学习中的图级预测任务。与其设计特征向量，不如设计核函数：

- k(G,G′)∈R
    
- 核矩阵 K=[K(G,G′)]G,G′​ 必须始终是半正定的。
    
- 存在一个特征表示 ϕ，使得 K(G,G′)=ϕ(G)⋅ϕ(G′)，这甚至可以是无限维的。
    

我们可以使用“词袋”（Bag-of-Words, BOW）来表示图。回想在自然语言处理中，BOW 仅使用单词计数作为文档的特征，而不考虑顺序。我们将节点视为“单词”。图级图元特征计算图中不同图元的数量。这里的图元与节点级特征中的定义略有不同，它们不是根节点，也不需要是连通的。这一定义的局限性在于，计算图元是昂贵的。通过枚举计算大小为 k 的图元在大小为 n 的图中需要 O(nk) 时间，因为子图同构测试成本高昂。如果图的节点度数有界，则时间可以压缩到 O(ndk−1)。到目前为止，我们只考虑了与图结构相关的特征，而没有考虑节点及其邻居的属性。

---

希望以上翻译对你有帮助！如果需要进一步的内容翻译，请随时告知。