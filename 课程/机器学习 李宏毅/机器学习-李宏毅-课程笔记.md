
# 04-Self-attention

## 4.1. 输入是向量序列
### 4.1.1 文字处理（自然语言处理）

**将每一个词汇表示为向量**。
- 一个很长的向量(one-hot编码)，长度与世界上存在的词汇数量一样多，所有词汇彼此之间没有关系。
- 给每一个词汇一个向量，这个向量包含语义信息，而一个句子就是一组长度不一的向量。
- ![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315225521042.png) ![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315225525789.png)

### 4.1.2 声音信号处理
会把一段声音信号取一个范围，这个范围叫做一个窗口（window），把该窗口里的信息描述成一个向量，这个向量称为一帧（frame）。一小段声音信号，它里面包含的信息量非常可观。
![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315225532722.png)

### 4.1.3 图
社交网络是一个图，在社交网络上，每一个节点就是一个人。每一个节点可以看作是一个向量。每个人的信息（性别、年龄及工作等）都可以用一个向量来表示。因此一个社交网络可以看做是一堆向量所组成。
![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315225538709.png)
把一个分子当作是模型的输入，每一个分子可以看作是一个图，分子上的每一个球就是一个原子，每个原子就是一个向量，而每个原子可以用one-hot向量来表示。
![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315225646173.png)
## 4.2. 输出的三种可能性
### 每一个向量都有一个对应的标签
输入与输出的长度是一样的。模型不需要去烦恼要输出多少个标签，输出多少个标签。
![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315225658279.png)
**例**：
- 词性标注（POS tagging）：机器会自动决定每一个词汇的词性，判断该词是名词、动词还是形容词等。
- 语音识别
- 社交网络：每个节点（人）进行标注【是否推送商品】

### 一组向量序列输出一个标签
整个序列只需要输出一个标签就好。
![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315225710388.png)
举例：
- 文本情感分析：给机器看一段话，模型要决定这段话是积极的（positive）还是消极的（negative）。
- 语音识别

### 模型自行决定输出多少个标签
输入是N个向量，输出可能是N'个标签，而N'是机器自己决定的。此种任务被称作**序列到序列（Sequence to Sequence，Seq2Seq）**。
![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315225737732.png)
举例：
- 翻译
- 语音识别

## 4.3. Self-attention 运作原理
### 4.3.1 以 Sequence Labeling 为例
> 考虑第一个输出可能性，每一个向量都有一个对应的标签，Sequence Labeling 要给序列里面的每一个向量一个标签。

#### 方法一

对每一个向量，用 Fully-connected network(或者说MLP多层感知机) 分别进行处理。
![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315225807303.png)

**问题**: 这种方法忽略了序列上下文的关系。同一个词汇在句子中不同的位置、不同的上下文环境下，词汇的词性有可能是不一样的，但此方法的输出会因是**同一个词汇而永远只有同一个输出**。

#### 方法二
改进方法一，串联若干个向量后丢进 Fully-connected network。给 Fully-connected network 一个整个 window 的信息，让它可以考虑一些上下文，即与该向量相邻的其他向量的信息。
![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315230142681.png)
**问题**: 序列的长度有长有短，输入给模型的序列的长度每次可能都不一样。开一个 window 比最长的序列还要长，才可能把整个序列盖住。但是开一个大的窗口，意味着 Fully-connected network 需要非常多的参数，可能运算量会很大，此外还容易过拟合。

⇒ 想要更好地考虑整个输入序列的信息，就要用到==**自注意力模型**==。

### 4.3.2 Self-attention model
考虑整个序列的所有向量，**综合向量序列整体和单个向量个体**，得到对每一个向量处理后的向量，将这些向量分别连接一个 FC，FC 可以专注于处理这一个位置的向量，得到对应结果。
![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315230311930.png)
自注意力模型不是只能用一次，可以叠加很多次，与 FC 可以交替使用。

#### 内部架构
![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315230417313.png)
**输入**：一串的 vector，这些 vector 可能是整个 network 的 input，也可能是某个 hidden layer 的 output。  
**输出**：处理 input 以后，**每一个 b 都是考虑了所有的 a 以后才生成出来的**。

#### 具体步骤
1. 根据 $a^1$ 向量找出跟其他向量的相关程度 $α$ (标量)![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315230621898.png)
2. 藉由一个计算 attention 的模块来得到 $α$ ![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315230634385.png)
   - **Dot-product**：把输入的两个向量分别乘上 $W^q$ 和 $W^k$，得到两个向量 $q$ 和 $k$ 后做点积，把它们做逐元素（element-wise）的相乘，再全部加起来得到一个 $α$**（常用，也被用在 Transformer 中）**。  
   - **Additive**：两个向量通过 $W^q$ 和 $W^k$ 得到 $q$ 和 $k$ 后，把 $q$ 和 $k$ 串联起来丢到激活函数（activation function），再乘上矩阵 $W$ 得到 $α$。
3. 计算完 $a^1$ 跟其他向量的相关性 $α$ 后（也必须计算 $a^1$ 跟自己的 $α$），把所有的 $α$ 经过 softmax（也可使用其他激活函数，如 ReLu）得到 $α'$。![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315230724868.png)
4. 把向量 $a^1$ 到 $a^4$ 乘上 $W^v$ 得到新的向量 $v^1, v^2, v^3, v^4$，接下来把每一个向量都去乘上 $α'$ 后再求和得到 $b^1$。![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315230745523.png)
	- 如果 $a^1$ 跟 $a^2$ 有高相关性，即 $α_{1,2}'$ 的值很大，再做加权和后，得到的 $b^1$ 就可能会比较接近 $v^2$。所以谁的注意力的分数最大，谁的 $v$ 就会主导（dominant）抽出来的结果。
	- **注意：$b^1$ 到 $b^4$ 是同时被计算出来的。** 没有数据依赖
$$
\begin{aligned}
b^1&=\alpha'_{1,1}v^1+\alpha'_{1,2}v^2+\alpha'_{1,3}v^3+\alpha'_{1,4}v^4\\
&=(q^1\cdot k^1)\times W^va^1+\dots+(q^4\cdot k^4)\times W^va^4 \\
&=(W^qa^1\cdot W^ka^1)\times W^va^1+\dots+(W^qa^4\cdot W^ka^4)\times W^va^4
\end{aligned}
$$


#### 矩阵的角度
1. 先计算 $Q = A \times W^q$，$K = A \times W^k$，$V = A \times W_v$，合并后以 $Q, K, V$ 表示
   ![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315233154840.png)
2. 根据 $Q , K^T$ 计算 $A$，经过一个激活函数（如 softmax 或 ReLu），得到 $A'$（称做 attention matrix）。  
   ![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315234503722.png)
3. 再乘以 $V$ 得到 $O$，以 $O$ 表示。  
   ![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315234509514.png)
4. 综合：     ![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315234836084.png)
   - $I$ 是 Self-attention 的一组 vector input。  
   - $I$ 分别乘上 $W^q, W^k, W^v$ 矩阵得到 $Q, K, V$。  
   - 接下来 $Q \times K^T$ 得到 $A$，再经过激活函数得到 $A'$ Attention Matrix（生成 $Q$ 就是为了得到 attention 的 score）。  
   - $A'$再乘上 $V$，就得到 $O$。$O$ 就是 Self-attention 这个 layer 的输出。
   - $W^q, W^k, W^v$ 是三个要学习的矩阵参数。

### 4.3.3 Multi-head Self-attention

Multi-head Self-attention 的使用非常广泛，有一些任务，如翻译、语音识别等，用该方法可以得到较好的结果。**需要多少的 head 是需要调的 hyperparameter**。  
**原因**：
在使用 Self-attention 计算相关性的时候，是用 $q$ 去找相关的 $k$。但是“相关”有很多种不同的形式，所以也许可以有多个 $q$，不同的 $q$ 负责不同种类的相关性，这就是 Multi-head Self-attention。
![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315235217503.png)
**步骤**：
1. 先把 $a$ 乘上一个矩阵得到 $q$。  
2. 再把 $q$ 乘上另外两个矩阵，分别得到 $q^1, q^2$ 和 $k^1, k^2$，代表有两个 head；同理可以得到 $v^1, v^2$。  ![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315235529289.png)
3. 从同一个 head 里的 $q,k,v$ 计算 $b$。  
4. 将各个 head 计算得到的 $b$ 拼接，通过一个 transform 得到 $O$，然后再送到下一层。![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315235610932.png)

## 4.4. Positional Encoding
到目前为止，Self-attention 的操作里面没有位置的信息，但有时候**位置的信息很重要**。
- 举例，在做词性标注时，动词较不容易出现在句首，如果某一个词汇是放在句首，其为动词的可能性就比较低，所以位置的信息往往也是有用的。

**方法**：
每个位置用一个 vector 来表示它是 sequence 的第 $i$ 个，然后拼接到原向量中。![](机器学习-李宏毅-课程笔记.assets/IMG-机器学习-李宏毅-课程笔记-20250315235717854.png)

产生 positional encoding vector 的方法有很多种，如人工设置、根据数据训练出来等，目前还不知道哪一种方法最好，仍是一个尚待研究的问题。

## 4.5. 应用
### 4.5.1 自然语言处理
在自然语言处理领域，除了 Transformer 外，BERT 也用到了 Self-attention。

### 4.5.2 语音
问题：把一段声音信号表示成一组向量的话，这组向量可能会非常地长；attention matrix 的计算复杂度是长度的平方，因此需要很大的计算量、很大的存储空间。  
解决方法：延伸 Self-attention 的概念，运用 Truncated Self-attention。使用 Truncated Self-attention 只考虑一个小范围语音，而不考虑一整个句子，如此就可以加快运算的速度。

### 4.5.3 图像
一张图像可以看作是一个向量序列，既然也是一个向量序列，那么就可以用 Self-attention 来处理图像。

#### Self-attention vs CNN
- **Self-attention**：考虑一个像素和整张图片的信息。⇒ 自己学出 receptive field 的形状和大小。  
- **CNN**：receptive field 是人为设定的，只考虑范围内的信息。  
结论：CNN 就是 self-attention 的特例，可以说是更 flexible 的 CNN。Self-attention 只要设定合适的参数，它可以做到跟 CNN 一模一样的事情。根据论文《An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale》显示的结果，给出以下解释：  
- Self-attention 弹性比较大，所以需要比较多的训练资料，训练资料少的时候会 overfitting。  
- 而 CNN 弹性比较小，在训练资料少时结果比较好，但训练资料多时，它没有办法从更多的训练资料得到好处。

#### Self-attention vs RNN
Recurrent Neural Network 跟 Self-attention 做的事情非常像，它们的 input 都是一个 vector sequence，前一个时间点的输出也会作为输入丢进 RNN 产生新的向量，也同时会输入到 FC。很多的应用往往都把 RNN 的架构逐渐改成 Self-attention 的架构。

主要区别：
- 对 RNN 来说，假设最右边的 vector 要考虑最左边的输入，那它必须要把最左边的输入存在 memory 中都不能够忘掉一路带到最右边，才能在最后的时间点被考虑。  
- 对 Self-attention 来说没有这个问题，它可以在整个 sequence 上非常远的 vector 之间轻易地抽取信息。  
- Self-attention 可以平行处理所有的输出，效率更高：Self-attention 四个 vector 是平行产生的，不需等谁先运算完才把其他运算出来。  
- RNN 无法平行化，必须依次产生。

### 4.5.4 图
Self-attention 也可以在图中使用，把 node 当作 vector。然而，图中的 edge 意味着节点之间的关系，所以我们可以只计算有 edge 相连的 node 的 attention，若两个 node 之间没有 edge，代表两个 node 没有关系，就不必计算 attention。这种方法也被称为图神经网络（GNN）。

## 4.6. Learn More
Self-attention 有多种变形，由于其计算成本高，减少其计算量是未来的研究方向。论文《Long Range Arena: A Benchmark for Efficient Transformers》比较了各种不同的自注意力的变形，许多 Self-attention 的变形如：Linformer、Performer、Reformer 等等，往往比原来的 Transformer 性能还能差一些，但是速度会比较快。想进一步研究可参考论文《Efficient Transformers: A Survey》。

# 05-Transformer

## 1. Seq2Seq 模型
Transformer 是一个序列到序列（Sequence-to-Sequence，Seq2Seq）的模型。序列到序列模型输入和输出都是一个序列，输入与输出序列长度之间的关系有两种情况：一是输入跟输出的长度一样；二是机器自行决定输出的长度。

## 2. 应用
### 2.1 语音识别
输入是声音信号的一串 vector，输出是语音辨识的结果，也就是输出的这段声音信号，所对应的文字，输出的长度由机器自己决定。

### 2.2 机器翻译
机器读一个语言的句子，输出另外一个语言的句子，输入的文字的长度是 \(N\)，输出的句子的长度是 \(N'\)， \(N\) 跟 \(N'\) 之间的关系也由机器自己来决定。

### 2.3 语音翻译
将听到的英文的声音信号翻译成中文文字。

**问题**：把语音识别系统跟机器翻译系统接起来就直接是语音翻译，为什么还需独立出语音翻译？
**答案**：因为世界上很多语言没有文字，无法做语音识别。因此需要对这些语言做语音翻译，直接把它翻译成文字。

### 2.4 语音合成
输入文字、输出声音信号就是语音合成（Text-To-Speech，TTS）。现在还没有真的做端到端（end-to-end）的模型，以闽南语的语音合成为例，其使用的模型还是分成两阶，首先模型会先把中文的文字转成闽南语的拼音，再把闽南语的拼音转成声音信号。

### 2.5 聊天机器人
因为聊天机器人的输入输出都是文字，文字是一个向量序列，所以可用序列到序列的模型来做一个聊天机器人。

### 2.6 QA 任务
很多自然语言处理的任务都可以想成是问答（Question Answering，QA）的任务，如：
- 翻译
- 自动摘要
- 情感分析

### 2.7 文法剖析
文法剖析任务中，输入是一段文字，输出是一个树状的结构，而一个树状的结构可以看成一个序列，该序列代表了这个树的结构。把树的结构转成一个序列以后，就可以用 Seq2seq 模型来做文法剖析，具体可参考论文：Grammar as a Foreign Language。

### 2.8 多标签（Multi-label）分类
**区分**：
- Multi-class：从数个 class 里面选择某一个 class。
- Multi-label：同一个 sample 可以属于多个 class。

多标签分类（multi-label classification）问题不能直接把它当作一个多分类问题来解。就算取一个 threshold，只输出分数最高的前三名，但因 sample 对应的类别的数量可能根本不一样，因此需要用 Seq2seq 模型，让机器自己决定输出类别的数量。可参考论文：End-to-End Object Detection with Transformers。

## 3. Transformer 架构
一般的 Seq2Seq 模型会分成 encoder 和 decoder，encoder 负责处理输入的序列，再把处理好的结果给 decoder 决定要输出的序列。

### 3.1 Encoder
编码器要做的事情就是给一排向量，输出另外一排向量。自注意力、循环神经网络（Recurrent Neural Network，RNN）、卷积神经网络都能输入一排向量，输出一排向量。Transformer 的编码器使用的是自注意力，输入一排向量，输出另外一个同样长度的向量。

#### 3.1.1 内部剖析
Encoder 中会分成很多的 block，每一个 block 都是输入一排向量，输出一排向量。最后一个 block 会输出最终的向量序列。Encoder 的每个 block 并不是神经网络的一层，在每个 block 中，输入一排向量后做 Self-attention，考虑整个序列的信息，输出另外一排向量。接下来这排向量会进到 FC，输出另外一排向量，这一排向量就是一个 block 的输出。

#### 3.1.2 Transformer 的 Encoder
Transformer 做的事情是更复杂的，因为 Transformer 加入了 residual connection 和 layer normalization 的设计。

步骤：
1. 考虑全部向量经由 Self-attention 得到输出向量 \(a\)，向量 \(a\) 加上其输入向量 \(b\) 得到新的输出，称为 residual connection。
2. 计算输入向量的 mean 和 standard deviation，做 layer normalization。
3. 得到的输出作为 FC 的输入，FC 输出结果和原输入做 residual connection，再做一次 layer normalization 得到的输出就是 Transformer Encoder 中一个 block 的一个输出向量。

N 表示 N 个 block。首先在输入需要加上 positional encoding。Multi-head attention 就属 Self-attention。过后再做 residual connection 和 layer normalization，接下来还要经过 FC，接着再做一次 residual connection 和 layer normalization。如此是一个 block 的输出，总共会重复 N 次。

### 3.2 Decoder
#### 3.2.1 Autoregressive（AT）
以 encoder 的向量为输入，并加上特殊的 token 符号 <BOS>（Begin Of Sequence）。在 NLP 中，每一个 token 都可以用一个 one-hot vector 表示，其中一维是 1，剩余都是 0。

步骤：
1. 向 decoder 输入 encoder 产生的向量。
2. 在 decoder 可能产生的文字里面加上特殊 token <BOS>。
3. decoder 输出一个向量（长度与 vocabulary size 一样），随后通过 softmax，挑选分数最高的一个字作为最终的输出。
4. 将 3. 的输出作为 decoder 新的输入。
5. 重复步骤 3. 和 4.
6. 从 vocabulary 中挑到 <EOS> token，让 decoder 停止。

decoder 的输入是它在前一个时间点的输出，其会把自己的输出当做接下来的输入，因此当 decoder 产生一个句子时，有可能看到错误的东西，造成 error propagation。

#### 3.2.2 Transformer 的 decoder
除了中间的部分，encoder 跟 decoder，并没有什么差别。最后会再做一个 softmax，使得它的输出变成一个概率分布，最主要差别是 decoder 的第一个 self-attention 是使用 masked multi-head attention。

**Masked Multi-Head Attention**：
产生的输出并不考虑右边的部分，原因是因为 decoder 输出原理是顺次产生。

#### 3.2.3 Non-autoregressive（NAT）
NAT 不是依次产生，而是一次吃一整排的 <BOS> Token，把整个句子一次性产生出来。

**问题**：如何确定 <BOS> 的个数？
**答案**：另外训练一个 classifier，吃 Encoder 的输入，输出一个数字，代表 decoder 应该要输出的长度。给很多个 <BOS> 的 token，例如 300 个 <BOS> 然后就会输出 300 个字。什么地方输出 <EOS> 表示这个句子结束的点。

**NAT 的好处**：
- **平行化**：NAT 的 decoder 不管句子的长度如何，都是一个步骤就产生出完整的句子，所以在速度上 NAT 的 decoder 比 AT 的 decoder 要快。
- **容易控制输出长度**：例如语音合成有一个 classifier 决定 NAT 的 decoder 应该输出的长度，并以此调整语音的速度。如果要让系统讲快一点，那就把 classifier 的 output 除以二，如此讲话速度就变两倍快。

NAT 的 decoder 的 performance 往往都比 AT 还差，原因：Multi-Modality。

### 3.3 Encoder-Decoder 的 CrossAttention
两个输入来自 Encoder（Encoder 提供两个箭头），Decoder 提供了一个箭头。

细节：
1. encoder 输入一排向量，输出一排向量 \(a_1, a_2, a_3\)，产生 \(k_1, k_2, k_3\) 及 \(v_1, v_2, v_3\)。
2. decoder 输入 <BOS> 经过 self-attention（masked）得到一个向量 \(q\)，乘上一个矩阵得到 \(q\)。
3. 利用 \(q\) 计算 attention 的分数，并做 normalization，得到 \(\alpha_1', \alpha_2', \alpha_3'\)。
4. 与 \(v_1, v_2, v_3\) 做 weighted sum 得到 \(v\)。
5. 将 \(v\) 输入至 FC 做接下来的任务。

总而言之，decoder 就是产生一个 \(q\)，去 encoder 抽取信息出来当做接下来 decoder 的 FC 的 Input。decoder 可以看 encoder 中的许多层而不一定只是最后一层：Rethinking and Improving Natural Language Generation with Layer-Wise Multi-View Decoding。

## 4. Transformer 训练过程

训练资料：一段音频与对应的文字，文字为 one-hot encoding 的向量。

训练过程：decoder 输出的是概率分布，可以通过输出的概率分布与 ground truth 之间的计算 cross entropy 并求梯度实现优化，使 cross entropy 的值越小越好。

**注意**：在训练 decoder 时，输入的是正确答案（ground truth）而不是自己产生的答案，称为 Teacher Forcing。

## 5. Seq2Seq 模型训练技巧
### 5.1 Copy Mechanism
decoder 没有必要自己创造输出，它需要做的事情是从输入的资料中复制一些东西出来，而不是“创造词汇”。

举例：
- Chatbot
- Summarization

### 5.2 Guided Attention
目的：强迫模型一定要把输入的每一个东西都看一遍（如 TTS），强迫 attention 要有固定的方式。

动机：Seq2Seq Model 有时候 Train 会产生莫名其妙的结果，比如漏字，例如：对语音合成或者是语音辨识来说，我们想象中的 attention，应该要由左向右，但有可能模型跳着看。

更多资讯：Monotonic Attention、Location-aware Attention。

### 5.3 Beam Search
每次找分数最高的词元来当做输出的方法称为 greedy decoding。红色路径就是通过 greedy decoding 得到的路径。但贪婪搜索不一定是最好的方法，红色路径第一步好，绿色路径一开始比较差，但最终结果是绿色路径比较好。

Beam Search 用比较有效的方法找一个估计的 solution、一个不是完全精准的 solution，这个方法有时候有用，有时候没有用，因为找出分数最高的路不一定比较好，取决于任务本身的特性。

假设任务的答案非常明确，比如语音识别，说一句话，识别的结果就只有一个可能。对这种任务而言，通常 Beam Search 就会比较有帮助；但如果任务需要模型发挥一点创造力，Beam Search 可能比较没有帮助。

### 5.4 加入 Noise
语音合成模型训练好以后，测试时要加入一些 noise。用正常的解码的方法产生出来的声音听起来不太像人声，产生出比较好的声音是需要一些随机性的，所以加入一些随机性的结果反而会比较好。

### 5.5 Scheduled Sampling
测试时，decoder 看到的是自己的输出，因此它会看到一些错误的东西。但是在训练的时候，decoder 看到的是完全正确的，这种不一致的现象叫做 exposure bias。

问题：因为 decoder 从来没见过错的东西，它看到错的东西会非常惊奇，接下来它产生的结果可能都会错掉，导致一步错步步错。

解决：给 decoder 的输入加一些错误的东西，模型反而会学得更好 ⇒ Scheduled Sampling。