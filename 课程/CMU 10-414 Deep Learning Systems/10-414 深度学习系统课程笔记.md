笔记内容由课程ppt+llm生成, 并进行一定修改

提示词: 
```markdown
我有一份英文课程 PPT，内容涉及 [课程主题]。请根据 PPT 内容，整理一份完整的中文笔记，要求如下：

- 用 `markdown` 代码块输出
- 公式使用 LaTeX 语法（用 `$$` 包裹）
- 笔记需要涵盖 PPT 的所有重要内容，包括概念、定理、例子、图表信息等
- 结构清晰，适当使用标题（`#`）、列表（`-`）、代码块等 Markdown 语法
```
# Lec2 机器学习回顾/Softmax回归

## 2.1 机器学习基础
### 数据驱动编程范式
- ​**传统编程困境**：手工编写分类逻辑困难（如手写数字识别）
- ​**监督学习解决方案**：
  1. 收集带标签的训练集 ${(x^{(i)}, y^{(i)})}_{i=1}^m$
  2. 通过机器学习算法自动生成分类程序

### 机器学习三要素
1. ​**假设类 (Hypothesis Class)** : 参数化映射函数 $h_\theta: \mathbb{R}^n \rightarrow \mathbb{R}^k$，描述输入到输出的关系
2. ​**损失函数 (Loss Function)** ​: 量化模型预测效果：$\ell(h(x), y) \rightarrow \mathbb{R}^+$
3. **​优化方法 (Optimization Method) ​**: 最小化经验风险$\min_\theta \frac{1}{m}\sum_{i=1}^m \ell(h_\theta(x^{(i)}), y^{(i)})$ 
---
## 2.2 Softmax回归示例
### 多类分类问题设定
- ​**输入空间**: $x^{(i)} \in \mathbb{R}^n$（如MNIST图像 $n=784$）
- ​**输出空间**: $y^{(i)} \in \{1,...,k\}$ （如MNIST有 $k=10$ 类）
- ​**训练集规模**: $m$ 个样本（如MNIST $m=60,000$）

### 线性假设函数
$$ h_\theta(x) = \theta^T x $$
其中
- 参数矩阵 $\theta \in \mathbb{R}^{n \times k}$, 输入参数x为n维列向量(n×1)
- 将输入的n维列向量映射为k维输出(如每个类别的概率)

**批量计算形式**：
$$ 
\mathbf X \in \mathbb{R}^{m \times n} = \begin{bmatrix} -x^{(1)T}- \\ \vdots \\ -x^{(m)T}- \end{bmatrix}, \quad
y\in\{1,\dots,k\}^m=\begin{bmatrix}y^{(1)}\\ \vdots \\ y^{(m)}
\end{bmatrix}
$$
- $-x^{(1)T}-$意为**强调**这个向量占一行
这样, 线性假设可以用批量计算形式写成: 
$$
h_\theta(X) = \mathbf X\theta 
  =\begin{bmatrix} -x^{(1)T}\theta- \\ \vdots \\ -x^{(m)T}\theta- \end{bmatrix}
  =\begin{bmatrix} -h_{\theta}(x^{(1)})^{T}- \\ \vdots \\ -h_{\theta}(x^{(m)})^T- \end{bmatrix}
$$
### 损失函数设计
1. ​**分类错误率**​（不可导，不适于优化）：
   $$\ell_{err}(h(x),y) = \begin{cases} 
   0 & \text{if } \arg\max_i h_i(x) = y \\
   1 & \text{otherwise}
   \end{cases}$$
2. ​**交叉熵损失**​（Softmax损失）：
	$$
	\begin{aligned}
	   z_i &= p(label=i) = \frac{\exp(h_i(x))}{\sum_{j=1}^k \exp(h_j(x))} \\
	   \ell_{ce}(h(x),y) &= -\log z_y = -h_y(x) + \log\sum_{j=1}^k \exp\left(h_j(x)\right)
	\end{aligned}
	$$
	- 注: 这里的log默认为e为底, 即ln

## 2.3 优化方法
### 梯度下降算法

对于函数$f: \mathbb R^{n\times k}\rightarrow \mathbb R$, 梯度定义为
$$
\nabla_\theta f(\theta)\in \mathbb R^{n\times k}=
\begin{bmatrix} 
\frac{\partial f(\theta)}{\partial \theta_{11}} & \dots & \frac{\partial f(\theta)}{\partial \theta_{1k}} \\ 
\vdots &  & \vdots \\
\frac{\partial f(\theta)}{\partial \theta_{n1}} & \dots & \frac{\partial f(\theta)}{\partial \theta_{nk}} 
\end{bmatrix}
$$
- ![](10-414%20深度学习系统课程笔记.assets/IMG-10-414%20深度学习系统课程笔记-20250304210853564.png)
- 在θ点处的梯度表示在局部上, 哪个方向能使函数增长最快
为了最小化函数(如损失函数), 梯度下降算法使用负梯度方向, 逐步逼近最小值
$$\theta := \theta -\alpha \nabla_{\theta}f(\theta)$$
- α为步长/学习率, α的大小对收敛性/收敛速度有很大影响 ![](10-414%20深度学习系统课程笔记.assets/IMG-10-414%20深度学习系统课程笔记-20250304211947988.png)


参数更新规则：
$$ \theta := \theta - \alpha \nabla_\theta \left( \frac{1}{m}\sum_{i=1}^m \ell_{ce}(\theta^T x^{(i)}, y^{(i)}) \right) $$

### 随机梯度下降 (SGD)

每次都使用所有数据来计算梯度开销过大, 可以使用采样构成一个minibatch, 将这个minibatch计算的结果作为真实梯度的**近似**
$$
\begin{array}{l}{{\mathrm{Repeat}:~~}}\\ {{\text{Samplea~a~minibatch~of~data~}X\in\mathbb{R}^{B\times n},y\in\{1,\dots,k\}^{B}}}\\ {{\text{更新参数 }\theta:=\theta-{\frac{\alpha}{B}}\sum_{i=1}^{B}\nabla_{\theta}\ell{\big(}h_{\theta}(x^{(i)}),y^{(i)}{\big)}}}\end{array}
$$
- 这个近似操作可以理解为引入一些"噪声", 这在某些情况下甚至能带来一些优势
### softmax的梯度

对向量$h \in \mathbb R^k$: 
$$
\begin{aligned}
\frac{\partial\ell_{ce}(h,y)}{\partial h_{i}}&=\frac{\partial}{\partial h_{i}}\left(-h_{y}+\log\sum_{j=1}^{k}\exp h_{j}\right)\\
&=-1\{i=y\}+\frac{\exp h_{i}}{\sum_{j=1}^{k}\exp h_{j}}
\end{aligned}

$$
- log实际上是ln
- 用向量形式表示: $\nabla_{h}\ell(h, y)=z-e_{y}, z=normalize(exp(h))$ 

回到softmax: $\nabla_{\theta}\ell_{ce}(\theta^Tx, y)$ , θ是n×k的**矩阵**, $\theta^Tx$是k维列**向量**, 如何计算矩阵/向量的导数和链式法则?
- 假装一切都是标量(无视转置), 使用链式法则, 然后重新排列 / 转置矩阵or向量, 让计算结果的维度大小正确
![](10-414%20深度学习系统课程笔记.assets/IMG-10-414%20深度学习系统课程笔记-20250304222514624.png)
$$\nabla_\theta\ell_{ce}(\theta^Tx,y)\in\mathbb{R}^{n\times k}=x(z-e_y)^T$$
用**批量计算**形式$$\nabla_\theta\ell_{ce}(\mathbf X\theta,y)\in\mathbb{R}^{n\times k}=\mathbf X^T(Z-I_y),\quad Z=\mathrm{normalize}(\exp(\mathbf X\Theta))$$
重复, 直到参数/loss收敛
1. Iterative over minibatches $X\in\mathbb{R}^{B\times n},y\in\{1,...,k\}^B$ of training set
2. Update the parameters $\theta:=\theta-\frac{\alpha}{B}X^T(Z-I_y)$

# Lec3 手动构建神经网络

## 3.1 从线性到非线性假设类

### 线性分类器的局限性
- 线性假设类基本形式：
  $$h_{\theta}(x)=\theta^{T} x,\quad\theta\in \mathbb{R}^{n\times k}$$
  - 通过k个线性函数划分输入空间
  - 无法处理非线性分类边界（如螺旋形数据）
<figure style="display: flex; justify-content: space-around; align-items: center;">
	<img src="10-414%20深度学习系统课程笔记.assets/IMG-10-414%20深度学习系统课程笔记-20250305211219799.png" style="width: 33%; height: auto; margin: 0 10px;">
	<img src="10-414%20深度学习系统课程笔记.assets/IMG-10-414%20深度学习系统课程笔记-20250305211634230.png" style="width: 33%; height: auto; margin: 0 10px;">
	
</figure>

### 非线性特征
#### 核心思想
- 通过特征映射提升维度：
  $$
  \begin{array}{c}
  h_{\theta}(x)=\theta^{T}\phi(x) \\
  \phi:\mathbb{R}^{n}\rightarrow\mathbb{R}^{d},\ \theta\in\mathbb{R}^{d\times k}
  \end{array}
  $$
#### 特征构造方法对比
| 方法 | 描述 | 数学形式 | 局限性 |
|------|------|---------|--------|
| 线性特征 | 简单线性组合 | $\phi(x)=W^T x$ | 等价于单层线性分类器 |
| ​**非线性特征**​ | 激活函数变换 | $\phi(x)=\sigma(W^T x)$ | 可表达复杂边界 |
#### 关键非线性变换示例
- ​**随机傅里叶特征**：
  $$W\sim\mathcal{N}(0,1)\text{(随机高斯采样)},\ \sigma=\cos(\cdot)$$
- ​**可学习非线性特征**：（需通过反向传播训练$W$）
  $$\phi(x)=\text{ReLU}(W^T x)$$

---
## 3.2 神经网络

### 基本定义
- ​**神经网络**：由多个可微参数化函数（层）组成的假设类
- ​**深度网络**：多层级联的非线性变换（现代网络通常>3层）

### 神经网络结构
#### 两层神经网络
> 也称为单隐藏层网络
<div style="text-align: center;">
    <img src="10-414%20深度学习系统课程笔记.assets/IMG-10-414%20深度学习系统课程笔记-20250305214646239.png" style="width: 200px;">
</div>
$$
\begin{aligned}
h_{\theta}(x) &= W_{2}^{T}\sigma\left(W_{1}^{T} x\right) \\
\theta &= \{W_{1}\in \mathbb{R}^{n\times d}, W_{2}\in \mathbb{R}^{d\times k}\}
\end{aligned}
$$
- 批量矩阵形式：$h_\theta(\mathbf X)=\sigma\left(\mathbf X \mathbf W_1\right) \mathbf W_2$
	- $\mathbf X\in \mathbb R^{m\times n}, W_{1}\in \mathbb{R}^{n\times d}, W_{2}\in \mathbb{R}^{d\times k}$ 
#### 多层感知机（MLP）

<div style="text-align: center;">
	<img src="10-414%20深度学习系统课程笔记.assets/IMG-10-414%20深度学习系统课程笔记-20250305221004492.png" width="300">
</div>
- L层通用结构：
$$
\begin{aligned}
\mathbf Z_{i+1} &= \sigma_{i}\left(\mathbf Z_{i} \mathbf W_{i}\right),\quad i=1,\ldots,L \\
\mathbf Z_{1} &= \mathbf X \\
h_{\theta}(\mathbf X) &= \mathbf Z_{L+1}
\end{aligned}
$$
  - 维度关系：$\mathbf Z_i\in\mathbb{R}^{m\times n_i},\ \mathbf W_i\in\mathbb{R}^{n_i\times n_{i+1}}$
	  - Z被称为layer, activation, neuron, 是网络在不同阶段生成的中间特征
  - 典型激活函数：ReLU, Sigmoid, Tanh

### 深度网络优势分析
| 观点    | 支持/反对 | 说明            |
| ----- | ----- | ------------- |
| 生物相似性 | ❌     | 实际与大脑工作机制差异显著 |
| 电路效率  | ⚠️    | 理论优势难以实际验证    |
| 经验有效性 | ✅     | 固定参数量下表现更优    |

---
## 3.3 反向传播

### 梯度计算框架
- 优化目标：  $$\min_{\theta} \frac{1}{m}\sum_{i=1}^{m}\ell_{ce}\left(h_{\theta}(x^{(i)}), y^{(i)}\right)$$
  - 核心需求：计算$\nabla_\theta\ell_{ce}(h_{\theta}(x^{(i)}), y^{(i)})$ 

### 两层网络梯度推导
> 批量矩阵形式：$h_\theta(\mathbf X)=\sigma\left(\mathbf X \mathbf W_1\right) \mathbf W_2$
	- $\mathbf X\in \mathbb R^{m\times n}, W_{1}\in \mathbb{R}^{n\times d}, W_{2}\in \mathbb{R}^{d\times k}$ 

对于简单的两层网络, 梯度用批量矩阵形式写为: $\nabla_{W_{1}, W_{2}}\ell_{ce}(h_{\theta}(XW_1)W_{2}, y^{(i)})$ 

$$
\begin{aligned}
  \frac{\partial\ell_{ce}(\sigma(XW_1)W_2,y)}{\partial W_2} & =\frac{\partial\ell_{ce}(\sigma(XW_1)W_2,y)}{\partial\sigma(XW_1)W_2}\cdot\frac{\partial\sigma(XW_1)W_2}{\partial W_2} \\
 &  =(S-I_y)\cdot\sigma(XW_1),\quad[S=\mathrm{~softmax}(\sigma(XW_1)W_2)] \\ \\
 \frac{\partial\ell_{ce}(\sigma(XW_1)W_2,y)}{\partial W_1} &
 =\frac{\partial\ell_{ce}(\sigma(XW_1)W_2,y)}{\partial\sigma(XW_1)W_2} \cdot \frac{\partial\sigma(XW_1)W_2}{\partial \sigma(XW_{1})} \cdot \frac{\partial \sigma(XW_{1})}{\partial XW_{1}} \cdot \frac{{\partial XW_{1}}}{\partial W_{1}}\\
 & = 
 \underbrace{(S-I_{y})}_{m \times k}\cdot \underbrace{W_{2}}_{d\times k}\cdot \underbrace{\sigma'(XW_{1})}_{m\times d} \cdot \underbrace{X}_{m\times n}
\end{aligned}
$$


对$W_2$的梯度
$$
\nabla_{W_2}\ell = \sigma(XW_1)^T(S-I_y)
$$
- $S=\text{softmax}(\sigma(XW_1)W_2)$
- $I_y$为one-hot标签矩阵

对$W_1$的梯度
$$
\nabla_{W_1}\ell = \underbrace{X^T}_{n\times m}\left[ \underbrace{(S-I_y)W_2^T}_{m\times d} \circ \underbrace{\sigma'(XW_1)}_{m\times d} \right] \in \mathbb{R}^{n\times d}
$$
- $\circ$表示逐元素乘法(标量乘法)
- $\sigma'$为激活函数导数

### 通用反向传播算法

> 尽管用了简化, 但是计算梯度还是有些麻烦

#### 推导过程

前向传播递推式: $Z_{i+1} = \sigma_i(Z_iW_i) \in \mathbb R^{m\times n_{i+1}}\quad (i=1,...,L)$

前向传播最终输出$Z_{L+1}$损失函数对于$W_{i}$的梯度, 由链式法则可得: 
![](10-414%20深度学习系统课程笔记.assets/IMG-10-414%20深度学习系统课程笔记-20250306153440999.png)
- $G_{i}$为损失对中间输出$Z_{i}$的导数
可得
$$
\begin{aligned}
G_i
  &=G_{i+1}\cdot\frac{\partial Z_{i+1}}{\partial Z_i}=G_{i+1}\cdot\frac{\partial\sigma_i(Z_iW_i)}{\partial Z_iW_i}\cdot\frac{\partial Z_iW_i}{\partial Z_i}\\
  &= \underbrace{G_{i+1}}_{m\times n_{i+1}}\cdot\underbrace{\sigma^{\prime}(Z_iW_i)}_{m\times n_{i+1}}\cdot \underbrace{W_i}_{n_{i}\times n_{i+1}}\\
G_{i}&=\frac{{\partial \ell(Z_{L+1}, y)}}{\partial Z_{i}}= \nabla_{Z_{i}}\ell(Z_{L+1}, y)\in \mathbb R^{m\times n_{i}}\\
\text{整理得:}\\
  G_i &= (G_{i+1} \circ \sigma_i'(Z_iW_i)) W_i^T \\
\end{aligned}
$$
回到参数$W_{i}$的梯度更新: 
$$
\begin{aligned}
\frac{\partial\ell(Z_{L+1},y)}{\partial W_i}&=G_{i+1}\cdot \frac{\partial Z_{i+1}}{\partial W_{i}}=G_{i+1}\cdot\frac{\partial\sigma_i(Z_iW_i)}{\partial Z_iW_i}\cdot\frac{\partial Z_iW_i}{\partial W_i}\\
&=\underbrace{G_{i+1}}_{m\times n_{i+1}}\cdot\underbrace{\sigma^{\prime}(Z_iW_i)}_{m\times n_{i+1}}\cdot \underbrace {Z_i}_{m\times n_{i}}\\
\Longrightarrow &\nabla_{W_i}\ell(Z_{L+1},y)=Z_i^T\left(G_{i+1}\circ\sigma^{\prime}(Z_iW_i)\right) \in \mathbb R^{n_i\times n_{i+1}}
\end{aligned}
$$
#### 前向传播

$$
\begin{aligned}
Z_1 &= X \in \mathbb R^{m\times n} \\
Z_{i+1} &= \sigma_i(Z_iW_i) \in \mathbb R^{m\times n_{i+1}}\quad (i=1,...,L)
\end{aligned}
$$
#### 反向传播
1. 初始化梯度：   $G_{L+1} = \nabla_{Z_{L+1}}\ell = S-I_y$
2. 反向迭代：$G_i = (G_{i+1} \circ \sigma_i'(Z_iW_i)) W_i^T$
3. 参数梯度计算： $\nabla_{W_i}\ell = Z_i^T(G_{i+1} \circ \sigma_i'(Z_iW_i))$ 

### 计算图视角
![](10-414%20深度学习系统课程笔记.assets/IMG-10-414%20深度学习系统课程笔记-20250306171653489.png)
- ​**向量-雅可比积**vector Jacobian product：核心计算单元
  $$\frac{\partial Z_{i+1}}{\partial W_i} = \sigma_i'(Z_iW_i) \cdot Z_i$$
- 内存优化：缓存前向传播结果$Z_i, W_i$


## 关键公式总结
| 公式类型 | 表达式 |
|---------|--------|
| 前向传播 | $Z_{i+1} = \sigma_i(Z_iW_i)$ |
| 损失梯度 | $G_{L+1} = S - I_y$ |
| 反向梯度 | $G_i = (G_{i+1} \circ \sigma_i') W_i^T$ |
| 权重梯度 | $\nabla_{W_i} = Z_i^T(G_{i+1} \circ \sigma_i')$ |
